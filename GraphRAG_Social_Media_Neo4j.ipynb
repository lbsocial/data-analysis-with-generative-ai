{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/lbsocial/data-analysis-with-generative-ai/blob/main/GraphRAG_Social_Media_Neo4j.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "dd5a95af",
      "metadata": {
        "id": "dd5a95af"
      },
      "source": [
        "# ğŸ§  GraphRAG: Retrieval-Augmented Generation with Neo4j Knowledge Graph\n",
        "\n",
        "**Ask natural-language questions â€” get graph-powered answers.**\n",
        "\n",
        "In the previous notebook (`Social_Media_ETL_Neo4j_Python.ipynb`) we built a social media knowledge graph in Neo4j with **Users, Tweets, Hashtags, and Places**. Now we will layer **AI** on top of that graph to create a **GraphRAG** pipeline.\n",
        "\n",
        "**What is GraphRAG?**\n",
        "\n",
        "Traditional RAG retrieves relevant text chunks from a vector store. **GraphRAG** goes further â€” it combines:\n",
        "1. **Vector Search** â€” find semantically similar tweets using embeddings.\n",
        "2. **Graph Traversal** â€” follow relationships (who posted it, where, which hashtags) to enrich context.\n",
        "3. **LLM Generation** â€” pass the enriched context to a language model for a grounded answer.\n",
        "\n",
        "![GraphRAG Pipeline](https://github.com/lbsocial/data-analysis-with-generative-ai/blob/1660b372f63accb8e55ea4b439924ba764639182/image/Gemini_Generated_Image_vslfdxvslfdxvslf.png?raw=true)\n",
        "\n",
        "**What we will build:**\n",
        "| Step | Action |\n",
        "|------|--------|\n",
        "| 1 | Install dependencies |\n",
        "| 2 | Connect to Neo4j & Gemini |\n",
        "| 3 | Create vector embeddings for every Tweet |\n",
        "| 4 | Build a Neo4j Vector Index |\n",
        "| 5 | Implement pure Vector Search retrieval |\n",
        "| 6 | Implement GraphRAG retrieval (Vector + Graph Traversal) |\n",
        "| 7 | Build the full RAG pipeline with Gemini |\n",
        "| 8 | Compare Vector-only vs GraphRAG answers |\n",
        "| 9 | Interactive GraphRAG query |\n",
        "| 10 | Advanced â€” Cypher-Augmented Generation |\n",
        "| 11 | Geospatial queries & Geo-Augmented GraphRAG |---\n",
        "\n",
        "\n",
        "> **Why no LangChain?** Frameworks like LangChain provide convenient abstractions (`GraphCypherQAChain`, `Neo4jVector`, etc.) that can simplify production code. However, in this tutorial we use the **Neo4j Python driver** and **Google GenAI SDK** directly so you can see exactly how each piece works â€” embedding, vector search, graph traversal, prompt formatting, and LLM generation. Once you understand these building blocks, adopting a framework becomes much easier."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "46192788",
      "metadata": {
        "id": "46192788"
      },
      "source": [
        "## ğŸ› ï¸ Step 1: Install Dependencies"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "id": "960268cf",
      "metadata": {
        "id": "960268cf"
      },
      "outputs": [],
      "source": [
        "pip install neo4j google-genai -q"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2316f9be",
      "metadata": {
        "id": "2316f9be"
      },
      "source": [
        "## ğŸ”Œ Step 2: Connect to Neo4j & Gemini\n",
        "\n",
        "We need two connections:\n",
        "- **Neo4j** â€” our knowledge graph.\n",
        "- **Google Gemini** â€” for generating embeddings and LLM answers.\n",
        "\n",
        "> **Note:** Store your credentials as environment variables or in Colab Secrets."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4cec4559",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4cec4559",
        "outputId": "585b4c3b-d36f-4952-f438-25e8ee8727fd"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "âœ… Connected to Neo4j\n",
            "âœ… Connected to Gemini (Gemini 2.0 Flash)\n"
          ]
        }
      ],
      "source": [
        "from google.colab import userdata\n",
        "from neo4j import GraphDatabase\n",
        "from google import genai\n",
        "\n",
        "# â”€â”€ Neo4j Credentials (from Colab Secrets) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "NEO4J_URI = userdata.get('NEO4J_URI')\n",
        "NEO4J_PASSWORD = userdata.get('NEO4J_PASSWORD')\n",
        "NEO4J_AUTH = (\"neo4j\", NEO4J_PASSWORD)\n",
        "\n",
        "# â”€â”€ Gemini Client (from Colab Secrets) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "GOOGLE_API_KEY = userdata.get('GOOGLE_API_KEY')\n",
        "client = genai.Client(api_key=GOOGLE_API_KEY)\n",
        "\n",
        "# â”€â”€ Verify Neo4j Connection â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "driver = GraphDatabase.driver(NEO4J_URI, auth=NEO4J_AUTH)\n",
        "driver.verify_connectivity()\n",
        "print(\"âœ… Connected to Neo4j\")\n",
        "\n",
        "# â”€â”€ Verify Gemini Connection â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "test = client.models.get(model=\"gemini-2.0-flash\")\n",
        "print(f\"âœ… Connected to Gemini ({test.display_name})\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cca90c6f",
      "metadata": {
        "id": "cca90c6f"
      },
      "source": [
        "## ğŸ“Š Step 2b: Verify the Graph Data\n",
        "\n",
        "Let's confirm the graph we built in the ETL notebook is ready."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "id": "97025886",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "97025886",
        "outputId": "d42abde8-66d7-443c-85c1-18747403d8a1"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:neo4j.notifications:Received notification from DBMS server: <GqlStatusObject gql_status='01N00', status_description='warn: feature deprecated. CALL subquery without a variable scope clause is deprecated. Use CALL () { ... }', position=<SummaryInputPosition line=2, column=9, offset=9>, raw_classification='DEPRECATION', classification=<NotificationClassification.DEPRECATION: 'DEPRECATION'>, raw_severity='WARNING', severity=<NotificationSeverity.WARNING: 'WARNING'>, diagnostic_record={'_classification': 'DEPRECATION', '_severity': 'WARNING', '_position': {'offset': 9, 'line': 2, 'column': 9}, 'OPERATION': '', 'OPERATION_CODE': '0', 'CURRENT_SCHEMA': '/'}> for query: \"\\n        CALL {\\n            MATCH (t:Tweet) RETURN 'Tweet' AS label, count(t) AS count\\n            UNION ALL\\n            MATCH (u:User)  RETURN 'User'  AS label, count(u) AS count\\n            UNION ALL\\n            MATCH (h:Hashtag) RETURN 'Hashtag' AS label, count(h) AS count\\n            UNION ALL\\n            MATCH (p:Place) RETURN 'Place' AS label, count(p) AS count\\n        }\\n        RETURN label, count\\n    \"\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "â”€â”€ Graph Node Summary â”€â”€\n",
            "       Tweet: 100\n",
            "        User: 5\n",
            "     Hashtag: 4\n",
            "       Place: 5\n",
            "\n",
            "â”€â”€ Sample Tweets â”€â”€\n",
            "  @Eve_Sec from London, UK\n",
            "    \"Pandas and NumPy are essential tools for any data engineer. #Python #Cloud #AI...\"\n",
            "    Tags: ['python', 'cloud', 'ai', 'python', 'cloud', 'ai']\n",
            "\n",
            "  @Charlie_AI from London, UK\n",
            "    \"The new Large Language Models are hallucinating less and reasoning more. #AI #Ne...\"\n",
            "    Tags: ['ai', 'neo4j']\n",
            "\n",
            "  @Dave_Dev from London, UK\n",
            "    \"Automating my daily workflows with a simple Python script. #Python #Cloud #Neo4j...\"\n",
            "    Tags: ['python', 'cloud', 'neo4j']\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Quick health check â€” count nodes by label\n",
        "with driver.session(database=\"neo4j\") as session:\n",
        "    result = session.run(\"\"\"\n",
        "        CALL {\n",
        "            MATCH (t:Tweet) RETURN 'Tweet' AS label, count(t) AS count\n",
        "            UNION ALL\n",
        "            MATCH (u:User)  RETURN 'User'  AS label, count(u) AS count\n",
        "            UNION ALL\n",
        "            MATCH (h:Hashtag) RETURN 'Hashtag' AS label, count(h) AS count\n",
        "            UNION ALL\n",
        "            MATCH (p:Place) RETURN 'Place' AS label, count(p) AS count\n",
        "        }\n",
        "        RETURN label, count\n",
        "    \"\"\")\n",
        "    print(\"â”€â”€ Graph Node Summary â”€â”€\")\n",
        "    for record in result:\n",
        "        print(f\"  {record['label']:>10s}: {record['count']}\")\n",
        "\n",
        "# Sample one tweet with all its relationships\n",
        "with driver.session(database=\"neo4j\") as session:\n",
        "    result = session.run(\"\"\"\n",
        "        MATCH (u:User)-[:POSTED]->(t:Tweet)-[:LOCATED_AT]->(p:Place)\n",
        "        OPTIONAL MATCH (t)-[:TAGGED_WITH]->(h:Hashtag)\n",
        "        RETURN u.username AS user, t.text AS text,\n",
        "               p.name AS place, collect(h.name) AS hashtags\n",
        "        LIMIT 3\n",
        "    \"\"\")\n",
        "    print(\"\\nâ”€â”€ Sample Tweets â”€â”€\")\n",
        "    for record in result:\n",
        "        print(f\"  @{record['user']} from {record['place']}\")\n",
        "        print(f\"    \\\"{record['text'][:80]}...\\\"\")\n",
        "        print(f\"    Tags: {record['hashtags']}\\n\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c7d9a78d",
      "metadata": {
        "id": "c7d9a78d"
      },
      "source": [
        "---\n",
        "## ğŸ§¬ Step 3: Generate Vector Embeddings for Tweets\n",
        "\n",
        "We use Gemini's `gemini-embedding-001` model to convert every tweet's text into a **3072-dimensional vector**. This vector captures the *semantic meaning* of the text.\n",
        "\n",
        "We then write each embedding back to the `Tweet` node as a property called `embedding`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "id": "d545a106",
      "metadata": {
        "id": "d545a106"
      },
      "outputs": [],
      "source": [
        "EMBEDDING_MODEL = \"models/gemini-embedding-001\"\n",
        "EMBEDDING_DIM = 3072\n",
        "\n",
        "\n",
        "def get_embeddings(texts: list[str]) -> list[list[float]]:\n",
        "    \"\"\"Get Gemini embeddings for a batch of texts.\"\"\"\n",
        "    response = client.models.embed_content(\n",
        "        model=EMBEDDING_MODEL,\n",
        "        contents=texts,\n",
        "    )\n",
        "    return [emb.values for emb in response.embeddings]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "M8lPYW_b2yX8",
      "metadata": {
        "id": "M8lPYW_b2yX8"
      },
      "outputs": [],
      "source": [
        "# â”€â”€ Check if embeddings already exist â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "with driver.session(database=\"neo4j\") as session:\n",
        "    result = session.run(\"\"\"\n",
        "        MATCH (t:Tweet)\n",
        "        RETURN count(t) AS total_tweets,\n",
        "               count(t.embedding) AS tweets_with_embeddings\n",
        "    \"\"\")\n",
        "    record = result.single()\n",
        "    total_tweets = record[\"total_tweets\"]\n",
        "    tweets_with_embeddings = record[\"tweets_with_embeddings\"]\n",
        "\n",
        "print(f\"ğŸ“Š Tweet embedding status:\")\n",
        "print(f\"   Total tweets: {total_tweets}\")\n",
        "print(f\"   Tweets with embeddings: {tweets_with_embeddings}\")\n",
        "\n",
        "# â”€â”€ Only embed if needed â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "if tweets_with_embeddings == total_tweets and total_tweets > 0:\n",
        "    print(\"\\nâœ… All tweets already have embeddings! Skipping embedding step.\")\n",
        "    print(\"   ğŸ’¡ This saves significant API quota!\")\n",
        "else:\n",
        "    print(f\"\\nâ³ Need to embed {total_tweets - tweets_with_embeddings} tweets...\")\n",
        "    \n",
        "    # Fetch tweets that need embeddings\n",
        "    with driver.session(database=\"neo4j\") as session:\n",
        "        result = session.run(\"\"\"\n",
        "            MATCH (t:Tweet)\n",
        "            WHERE t.embedding IS NULL\n",
        "            RETURN t.id AS id, t.text AS text\n",
        "        \"\"\")\n",
        "        tweets = [(record[\"id\"], record[\"text\"]) for record in result]\n",
        "\n",
        "    if len(tweets) == 0:\n",
        "        print(\"âœ… No tweets need embedding!\")\n",
        "    else:\n",
        "        print(f\"ğŸ“„ Embedding {len(tweets)} tweets...\")\n",
        "        \n",
        "        # â”€â”€ Batch embed (20 tweets at a time) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "        BATCH_SIZE = 20\n",
        "        for i in range(0, len(tweets), BATCH_SIZE):\n",
        "            batch = tweets[i : i + BATCH_SIZE]\n",
        "            ids = [t[0] for t in batch]\n",
        "            texts = [t[1] for t in batch]\n",
        "\n",
        "            embeddings = get_embeddings(texts)\n",
        "\n",
        "            # Write embeddings back to Neo4j\n",
        "            with driver.session(database=\"neo4j\") as session:\n",
        "                session.run(\n",
        "                    \"\"\"\n",
        "                    UNWIND $data AS row\n",
        "                    MATCH (t:Tweet {id: row.id})\n",
        "                    SET t.embedding = row.embedding\n",
        "                    \"\"\",\n",
        "                    data=[{\"id\": id_, \"embedding\": emb} for id_, emb in zip(ids, embeddings)],\n",
        "                )\n",
        "\n",
        "            print(f\"  âœ… Embedded batch {i // BATCH_SIZE + 1}/{(len(tweets) - 1) // BATCH_SIZE + 1}\")\n",
        "\n",
        "        print(\"\\nğŸ‰ All tweet embeddings stored in Neo4j!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "47fc570a",
      "metadata": {
        "id": "47fc570a"
      },
      "source": [
        "---\n",
        "## ğŸ—‚ï¸ Step 4: Create a Neo4j Vector Index\n",
        "\n",
        "A **vector index** lets Neo4j perform fast approximate-nearest-neighbor (ANN) search on the embedding property. This is the foundation of our retrieval step."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "id": "25bb5bb8",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "25bb5bb8",
        "outputId": "afb15c73-a397-47af-c025-4bcad3968486"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Attempting to drop existing index 'tweet_embeddings' if it exists...\n",
            "âœ… Index 'tweet_embeddings' dropped (if it existed).\n",
            "âœ… Vector index 'tweet_embeddings' created with 3072 dimensions.\n",
            "  Index: tweet_embeddings\n",
            "  State: POPULATING\n",
            "  Index configuration properties not found.\n"
          ]
        }
      ],
      "source": [
        "INDEX_NAME = \"tweet_embeddings\"\n",
        "\n",
        "with driver.session(database=\"neo4j\") as session:\n",
        "    # Ensure the index is dropped idempotently\n",
        "    print(f\"Attempting to drop existing index '{INDEX_NAME}' if it exists...\")\n",
        "    session.run(f\"DROP INDEX {INDEX_NAME} IF EXISTS\")\n",
        "    print(f\"âœ… Index '{INDEX_NAME}' dropped (if it existed).\")\n",
        "\n",
        "    # Create the vector index with the correct dimensions\n",
        "    session.run(f\"\"\"\n",
        "        CREATE VECTOR INDEX {INDEX_NAME}\n",
        "        FOR (t:Tweet)\n",
        "        ON (t.embedding)\n",
        "        OPTIONS {{\n",
        "            indexConfig: {{\n",
        "                `vector.dimensions`: {EMBEDDING_DIM},\n",
        "                `vector.similarity_function`: 'cosine'\n",
        "            }}\n",
        "        }}\n",
        "    \"\"\")\n",
        "    print(f\"âœ… Vector index '{INDEX_NAME}' created with {EMBEDDING_DIM} dimensions.\")\n",
        "\n",
        "# Verify index details, specifically dimensions\n",
        "with driver.session(database=\"neo4j\") as session:\n",
        "    # Run SHOW VECTOR INDEXES and then filter in Python\n",
        "    result = session.run(\"SHOW VECTOR INDEXES\")\n",
        "    found_index = False\n",
        "    for record in result:\n",
        "        if record['name'] == INDEX_NAME:\n",
        "            found_index = True\n",
        "            print(f\"  Index: {record['name']}\")\n",
        "            print(f\"  State: {record['state']}\")\n",
        "            # Extract dimensions from the 'properties' map\n",
        "            if 'properties' in record and 'indexConfig' in record['properties']:\n",
        "                config = record['properties']['indexConfig']\n",
        "                dimensions = config.get('vector.dimensions')\n",
        "                if dimensions:\n",
        "                    print(f\"  Actual Dimensions: {dimensions}\")\n",
        "                else:\n",
        "                    print(\"  Dimensions not found in index properties.\")\n",
        "            else:\n",
        "                print(\"  Index configuration properties not found.\")\n",
        "            break\n",
        "    if not found_index:\n",
        "        print(f\"âŒ Index '{INDEX_NAME}' not found after creation.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fae6b082",
      "metadata": {
        "id": "fae6b082"
      },
      "source": [
        "---\n",
        "## ğŸ” Step 5: Pure Vector Search (Baseline)\n",
        "\n",
        "First, let's implement a plain **vector similarity search**. We embed the user's question, then find the `k` most similar tweets. This is what a traditional RAG system does â€” **no graph knowledge** is used."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "id": "cd877d2a",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cd877d2a",
        "outputId": "fc0aadea-8d77-4412-97e3-23dce3578bde"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ğŸ” Vector Search Results for: 'What are people saying about graph databases?'\n",
            "\n",
            "  1. [score=0.8845] Graph databases are game changers for handling complex relationships. #Neo4j\n",
            "  2. [score=0.8803] Graph databases are game changers for handling complex relationships. #Neo4j #AI\n",
            "  3. [score=0.8740] Graph databases are game changers for handling complex relationships. #Neo4j #Python\n",
            "  4. [score=0.8733] Graph databases are game changers for handling complex relationships. #Neo4j #AI #Python\n",
            "  5. [score=0.8547] Relational DBs struggle with joins, but graphs handle them naturally. #Neo4j\n"
          ]
        }
      ],
      "source": [
        "def vector_search(question: str, k: int = 5) -> list[dict]:\n",
        "    \"\"\"\n",
        "    Plain vector search â€” returns the top-k most similar tweets.\n",
        "    No graph traversal.\n",
        "    \"\"\"\n",
        "    q_embedding = get_embeddings([question])[0]\n",
        "\n",
        "    cypher = \"\"\"\n",
        "        CALL db.index.vector.queryNodes($index, $k, $embedding)\n",
        "        YIELD node AS tweet, score\n",
        "        RETURN tweet.id    AS id,\n",
        "               tweet.text  AS text,\n",
        "               tweet.likes AS likes,\n",
        "               score\n",
        "        ORDER BY score DESC\n",
        "    \"\"\"\n",
        "    with driver.session(database=\"neo4j\") as session:\n",
        "        result = session.run(cypher, index=INDEX_NAME, k=k, embedding=q_embedding)\n",
        "        return [dict(record) for record in result]\n",
        "\n",
        "\n",
        "# â”€â”€ Test it â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "question = \"What are people saying about graph databases?\"\n",
        "results = vector_search(question)\n",
        "\n",
        "print(f\"ğŸ” Vector Search Results for: '{question}'\\n\")\n",
        "for i, r in enumerate(results, 1):\n",
        "    print(f\"  {i}. [score={r['score']:.4f}] {r['text'][:90]}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7f5b073e",
      "metadata": {
        "id": "7f5b073e"
      },
      "source": [
        "---\n",
        "## ğŸ•¸ï¸ Step 6: GraphRAG Retrieval (Vector + Graph Traversal)\n",
        "\n",
        "This is the key innovation. After finding similar tweets via vector search, we **traverse the graph** to collect rich context:\n",
        "\n",
        "| Traversal | What we get |\n",
        "|-----------|-------------|\n",
        "| `Tweet â† POSTED â† User` | Who wrote it? How many followers? |\n",
        "| `Tweet â†’ LOCATED_AT â†’ Place` | Where was it posted? |\n",
        "| `Tweet â†’ TAGGED_WITH â†’ Hashtag` | What topics is it about? |\n",
        "| `User â†’ POSTED â†’ OtherTweets` | What else has this user said? (context expansion) |\n",
        "| `Hashtag â† TAGGED_WITH â† OtherTweets` | What else is tagged with the same topics? |\n",
        "\n",
        "This gives the LLM **much richer context** than bare text chunks."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "id": "51b3d6f7",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "51b3d6f7",
        "outputId": "33c04fe3-2e0f-4ebd-8cb5-ef81d2121344"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ğŸ•¸ï¸ GraphRAG Results for: 'What are people saying about graph databases?'\n",
            "\n",
            "  1. [score=0.8845] @Alice_Data (33440 followers)\n",
            "     ğŸ“ London, UK, GB\n",
            "     ğŸ’¬ \"Graph databases are game changers for handling complex relationships. #Neo4j...\"\n",
            "     ğŸ·ï¸  Tags: ['neo4j']\n",
            "     ğŸ”— Related: \"Pandas and NumPy are essential tools for any data engineer. ...\"\n",
            "\n",
            "  2. [score=0.8803] @Dave_Dev (40016 followers)\n",
            "     ğŸ“ San Francisco, CA, US\n",
            "     ğŸ’¬ \"Graph databases are game changers for handling complex relationships. #Neo4j #AI...\"\n",
            "     ğŸ·ï¸  Tags: ['ai', 'neo4j']\n",
            "     ğŸ”— Related: \"Pandas and NumPy are essential tools for any data engineer. ...\"\n",
            "\n",
            "  3. [score=0.8740] @Alice_Data (33440 followers)\n",
            "     ğŸ“ Tokyo, JP, JP\n",
            "     ğŸ’¬ \"Graph databases are game changers for handling complex relationships. #Neo4j #Py...\"\n",
            "     ğŸ·ï¸  Tags: ['python', 'neo4j']\n",
            "     ğŸ”— Related: \"Pandas and NumPy are essential tools for any data engineer. ...\"\n",
            "\n",
            "  4. [score=0.8733] @Alice_Data (33440 followers)\n",
            "     ğŸ“ Paris, FR, FR\n",
            "     ğŸ’¬ \"Graph databases are game changers for handling complex relationships. #Neo4j #AI...\"\n",
            "     ğŸ·ï¸  Tags: ['python', 'ai', 'neo4j']\n",
            "     ğŸ”— Related: \"Pandas and NumPy are essential tools for any data engineer. ...\"\n",
            "\n",
            "  5. [score=0.8547] @Eve_Sec (25197 followers)\n",
            "     ğŸ“ Tokyo, JP, JP\n",
            "     ğŸ’¬ \"Relational DBs struggle with joins, but graphs handle them naturally. #Neo4j...\"\n",
            "     ğŸ·ï¸  Tags: ['neo4j']\n",
            "     ğŸ”— Related: \"Pandas and NumPy are essential tools for any data engineer. ...\"\n",
            "\n"
          ]
        }
      ],
      "source": [
        "def graph_rag_search(question: str, k: int = 5) -> list[dict]:\n",
        "    \"\"\"\n",
        "    GraphRAG retrieval â€” vector search + graph traversal.\n",
        "    Returns enriched context for each matching tweet.\n",
        "    \"\"\"\n",
        "    q_embedding = get_embeddings([question])[0]\n",
        "\n",
        "    cypher = \"\"\"\n",
        "        // â”€â”€ 1. Vector Search: find semantically similar tweets â”€â”€\n",
        "        CALL db.index.vector.queryNodes($index, $k, $embedding)\n",
        "        YIELD node AS tweet, score\n",
        "\n",
        "        // â”€â”€ 2. Graph Traversal: enrich with relationships â”€â”€\n",
        "        // Get the author\n",
        "        MATCH (author:User)-[:POSTED]->(tweet)\n",
        "\n",
        "        // Get the location\n",
        "        OPTIONAL MATCH (tweet)-[:LOCATED_AT]->(place:Place)\n",
        "\n",
        "        // Get all hashtags on this tweet\n",
        "        OPTIONAL MATCH (tweet)-[:TAGGED_WITH]->(hashtag:Hashtag)\n",
        "\n",
        "        // â”€â”€ 3. Context Expansion: other tweets by same author â”€â”€\n",
        "        OPTIONAL MATCH (author)-[:POSTED]->(other_tweet:Tweet)\n",
        "        WHERE other_tweet.id <> tweet.id\n",
        "\n",
        "        // â”€â”€ 4. Context Expansion: co-occurring tweets via hashtags â”€â”€\n",
        "        OPTIONAL MATCH (hashtag)<-[:TAGGED_WITH]-(related_tweet:Tweet)\n",
        "        WHERE related_tweet.id <> tweet.id\n",
        "\n",
        "        RETURN tweet.id                        AS tweet_id,\n",
        "               tweet.text                      AS text,\n",
        "               tweet.likes                     AS likes,\n",
        "               tweet.retweets                  AS retweets,\n",
        "               score,\n",
        "\n",
        "               // Author context\n",
        "               author.username                 AS author,\n",
        "               author.followers                AS author_followers,\n",
        "\n",
        "               // Location context\n",
        "               place.name                      AS location,\n",
        "               place.country                   AS country,\n",
        "\n",
        "               // Hashtag context\n",
        "               collect(DISTINCT hashtag.name)   AS hashtags,\n",
        "\n",
        "               // Expanded context\n",
        "               collect(DISTINCT other_tweet.text)[0..3]   AS author_other_tweets,\n",
        "               collect(DISTINCT related_tweet.text)[0..3] AS related_tweets_via_hashtag\n",
        "\n",
        "        ORDER BY score DESC\n",
        "    \"\"\"\n",
        "    with driver.session(database=\"neo4j\") as session:\n",
        "        result = session.run(cypher, index=INDEX_NAME, k=k, embedding=q_embedding)\n",
        "        return [dict(record) for record in result]\n",
        "\n",
        "\n",
        "# â”€â”€ Test it â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "question = \"What are people saying about graph databases?\"\n",
        "results = graph_rag_search(question)\n",
        "\n",
        "print(f\"ğŸ•¸ï¸ GraphRAG Results for: '{question}'\\n\")\n",
        "for i, r in enumerate(results, 1):\n",
        "    print(f\"  {i}. [score={r['score']:.4f}] @{r['author']} ({r['author_followers']} followers)\")\n",
        "    print(f\"     ğŸ“ {r['location']}, {r['country']}\")\n",
        "    print(f\"     ğŸ’¬ \\\"{r['text'][:80]}...\\\"\")\n",
        "    print(f\"     ğŸ·ï¸  Tags: {r['hashtags']}\")\n",
        "    if r['related_tweets_via_hashtag']:\n",
        "        print(f\"     ğŸ”— Related: \\\"{r['related_tweets_via_hashtag'][0][:60]}...\\\"\")\n",
        "    print()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "dc78870a",
      "metadata": {
        "id": "dc78870a"
      },
      "source": [
        "---\n",
        "## ğŸ¤– Step 7: Build the Full RAG Pipeline with Gemini\n",
        "\n",
        "Now we connect everything: **retrieve** enriched context from the graph, **format** it into a prompt, and **generate** an answer with Gemini.\n",
        "\n",
        "We build two pipelines side-by-side:\n",
        "- `rag_answer()` â€” plain vector retrieval + Gemini\n",
        "- `graph_rag_answer()` â€” GraphRAG retrieval + Gemini"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "id": "40af5918",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "40af5918",
        "outputId": "e7b2be99-66b5-4d59-ca39-3c4bda264879"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "âœ… RAG pipelines ready.\n"
          ]
        }
      ],
      "source": [
        "LLM_MODEL = \"gemini-2.0-flash\"\n",
        "\n",
        "SYSTEM_PROMPT = \"\"\"You are a social media analyst assistant.\n",
        "Answer the user's question based ONLY on the retrieved context below.\n",
        "If the context doesn't contain enough information, say so.\n",
        "Always cite specific tweets, users, or locations when relevant.\n",
        "Be concise but thorough.\"\"\"\n",
        "\n",
        "\n",
        "def format_vector_context(results: list[dict]) -> str:\n",
        "    \"\"\"Format plain vector search results into a text block.\"\"\"\n",
        "    lines = []\n",
        "    for i, r in enumerate(results, 1):\n",
        "        lines.append(f\"Tweet {i} (similarity={r['score']:.3f}, likes={r['likes']}):\\n  \\\"{r['text']}\\\"\")\n",
        "    return \"\\n\\n\".join(lines)\n",
        "\n",
        "\n",
        "def format_graph_context(results: list[dict]) -> str:\n",
        "    \"\"\"Format GraphRAG results into a rich context block.\"\"\"\n",
        "    lines = []\n",
        "    for i, r in enumerate(results, 1):\n",
        "        block = f\"\"\"Tweet {i} (similarity={r['score']:.3f}):\n",
        "  Text: \"{r['text']}\"\n",
        "  Author: @{r['author']} ({r['author_followers']} followers)\n",
        "  Location: {r['location']}, {r['country']}\n",
        "  Engagement: {r['likes']} likes, {r['retweets']} retweets\n",
        "  Hashtags: {', '.join(r['hashtags'])}\"\"\"\n",
        "        if r.get('author_other_tweets'):\n",
        "            block += f\"\\n  Other tweets by @{r['author']}:\"\n",
        "            for t in r['author_other_tweets']:\n",
        "                block += f\"\\n    - \\\"{t[:80]}...\\\"\"\n",
        "        if r.get('related_tweets_via_hashtag'):\n",
        "            block += f\"\\n  Related tweets (same hashtags):\"\n",
        "            for t in r['related_tweets_via_hashtag']:\n",
        "                block += f\"\\n    - \\\"{t[:80]}...\\\"\"\n",
        "        lines.append(block)\n",
        "    return \"\\n\\n\".join(lines)\n",
        "\n",
        "\n",
        "def rag_answer(question: str, k: int = 5) -> str:\n",
        "    \"\"\"Traditional RAG: vector search + LLM.\"\"\"\n",
        "    results = vector_search(question, k)\n",
        "    context = format_vector_context(results)\n",
        "\n",
        "    response = client.models.generate_content(\n",
        "        model=LLM_MODEL,\n",
        "        contents=f\"Context:\\n{context}\\n\\nQuestion: {question}\",\n",
        "        config=genai.types.GenerateContentConfig(\n",
        "            system_instruction=SYSTEM_PROMPT,\n",
        "            temperature=0.2,\n",
        "        ),\n",
        "    )\n",
        "    return response.text\n",
        "\n",
        "\n",
        "def graph_rag_answer(question: str, k: int = 5) -> str:\n",
        "    \"\"\"GraphRAG: vector search + graph traversal + LLM.\"\"\"\n",
        "    results = graph_rag_search(question, k)\n",
        "    context = format_graph_context(results)\n",
        "\n",
        "    response = client.models.generate_content(\n",
        "        model=LLM_MODEL,\n",
        "        contents=f\"Context:\\n{context}\\n\\nQuestion: {question}\",\n",
        "        config=genai.types.GenerateContentConfig(\n",
        "            system_instruction=SYSTEM_PROMPT,\n",
        "            temperature=0.2,\n",
        "        ),\n",
        "    )\n",
        "    return response.text\n",
        "\n",
        "\n",
        "print(\"âœ… RAG pipelines ready.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a03fa285",
      "metadata": {
        "id": "a03fa285"
      },
      "source": [
        "---\n",
        "## âš–ï¸ Step 8: Compare Vector-Only RAG vs GraphRAG\n",
        "\n",
        "Let's ask the same questions and compare the quality of answers."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "af9459ed",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 369
        },
        "id": "af9459ed",
        "outputId": "2a2f3292-a104-4eb3-82db-795501d891ea"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "================================================================================\n",
            "â“ Question: What are people saying about graph databases and who are the most active users talking about them?\n",
            "================================================================================\n",
            "\n",
            "ğŸ“„ â”€â”€ TRADITIONAL RAG (Vector Only) â”€â”€\n",
            "âš ï¸ Error during Vector RAG: 429 RESOURCE_EXHAUSTED. {'error': {'code': 429, 'message': 'You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits. To monitor your current usage, head to: https://ai.dev/rate-limit. \\n* Quota exceeded for metric: generativelanguage.googleapis.com/generate_content_free_tier_input_token_count, limit: 0, model: gemini-2.0-flash\\n* Quota exceeded for metric: generativelanguage.googleapis.com/generate_content_free_tier_requests, limit: 0, model: gemini-2.0-flash\\n* Quota exceeded for metric: generativelanguage.googleapis.com/generate_content_free_tier_requests, limit: 0, model: gemini-2.0-flash\\nPlease retry in 16.439948258s.', 'status': 'RESOURCE_EXHAUSTED', 'details': [{'@type': 'type.googleapis.com/google.rpc.Help', 'links': [{'description': 'Learn more about Gemini API quotas', 'url': 'https://ai.google.dev/gemini-api/docs/rate-limits'}]}, {'@type': 'type.googleapis.com/google.rpc.QuotaFailure', 'violations': [{'quotaMetric': 'generativelanguage.googleapis.com/generate_content_free_tier_input_token_count', 'quotaId': 'GenerateContentInputTokensPerModelPerMinute-FreeTier', 'quotaDimensions': {'model': 'gemini-2.0-flash', 'location': 'global'}}, {'quotaMetric': 'generativelanguage.googleapis.com/generate_content_free_tier_requests', 'quotaId': 'GenerateRequestsPerMinutePerProjectPerModel-FreeTier', 'quotaDimensions': {'location': 'global', 'model': 'gemini-2.0-flash'}}, {'quotaMetric': 'generativelanguage.googleapis.com/generate_content_free_tier_requests', 'quotaId': 'GenerateRequestsPerDayPerProjectPerModel-FreeTier', 'quotaDimensions': {'location': 'global', 'model': 'gemini-2.0-flash'}}]}, {'@type': 'type.googleapis.com/google.rpc.RetryInfo', 'retryDelay': '16s'}]}}\n",
            "\n",
            "... (sleeping 30s to avoid rate limits) ...\n"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-3114950665.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     20\u001b[0m     \u001b[0;31m# â³ Pause to respect API rate limits\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"\\n... (sleeping 30s to avoid rate limits) ...\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m     \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msleep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m30\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"\\nğŸ•¸ï¸ â”€â”€ GRAPHRAG (Vector + Graph Traversal) â”€â”€\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "import time\n",
        "\n",
        "comparison_questions = [\n",
        "    \"What are people saying about graph databases and who are the most active users talking about them?\",\n",
        "    # \"Which cities generate the most discussion about AI and cloud computing?\",  # Uncomment to run more questions\n",
        "    # \"Are there any users who tweet about both Python and Neo4j? What are they saying?\",  # Uncomment to run more questions\n",
        "]\n",
        "\n",
        "for question in comparison_questions:\n",
        "    print(\"=\" * 80)\n",
        "    print(f\"â“ Question: {question}\")\n",
        "    print(\"=\" * 80)\n",
        "\n",
        "    print(\"\\nğŸ“„ â”€â”€ TRADITIONAL RAG (Vector Only) â”€â”€\")\n",
        "    try:\n",
        "        print(rag_answer(question))\n",
        "    except Exception as e:\n",
        "        print(f\"âš ï¸ Error during Vector RAG: {e}\")\n",
        "        if \"quota\" in str(e).lower() or \"429\" in str(e):\n",
        "            print(\"â³ Rate limit hit. Waiting 2 minutes before retrying...\")\n",
        "            time.sleep(120)\n",
        "\n",
        "    # â³ Pause to respect API rate limits (increased for free tier)\n",
        "    print(\"\\n... (sleeping 90s to avoid rate limits) ...\")\n",
        "    time.sleep(90)\n",
        "\n",
        "    print(\"\\nğŸ•¸ï¸ â”€â”€ GRAPHRAG (Vector + Graph Traversal) â”€â”€\")\n",
        "    try:\n",
        "        print(graph_rag_answer(question))\n",
        "    except Exception as e:\n",
        "        print(f\"âš ï¸ Error during GraphRAG: {e}\")\n",
        "        if \"quota\" in str(e).lower() or \"429\" in str(e):\n",
        "\n",
        "            print(\"â³ Rate limit hit. Waiting 2 minutes before continuing...\")    time.sleep(120)\n",
        "\n",
        "            time.sleep(120)    print(\"\\n... (sleeping 2 minutes before next question) ...\\n\")\n",
        "\n",
        "    # â³ Longer pause between questions (increased for free tier)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "42529791",
      "metadata": {
        "id": "42529791"
      },
      "source": [
        "---\n",
        "## ğŸ§ª Step 9: Interactive GraphRAG Query\n",
        "\n",
        "Try your own questions below!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "56517c87",
      "metadata": {
        "id": "56517c87"
      },
      "outputs": [],
      "source": [
        "# â”€â”€ Change this question to anything you want â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "your_question = \"What topics are trending in San Francisco?\"\n",
        "\n",
        "print(f\"â“ {your_question}\\n\")\n",
        "print(graph_rag_answer(your_question))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "755b4fcf",
      "metadata": {
        "id": "755b4fcf"
      },
      "source": [
        "---\n",
        "## ğŸ”¬ Step 10: Advanced â€” Cypher-Augmented Generation\n",
        "\n",
        "For structured analytical questions (\"How many tweets per city?\"), we can let the LLM **generate Cypher queries** directly. This combines the power of graph queries with natural language."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3b90f060",
      "metadata": {
        "id": "3b90f060"
      },
      "outputs": [],
      "source": [
        "CYPHER_SYSTEM_PROMPT = \"\"\"You are a Neo4j Cypher expert. Given the user's question, generate a Cypher query to answer it.\n",
        "\n",
        "The graph schema is:\n",
        "- (:User {id, username, name, followers, following, tweet_count})\n",
        "    -[:POSTED]->(:Tweet {id, text, created_at, likes, retweets, replies, location, embedding})\n",
        "- (:Tweet)-[:LOCATED_AT]->(:Place {name, country, location})\n",
        "- (:Tweet)-[:TAGGED_WITH]->(:Hashtag {name})\n",
        "\n",
        "Rules:\n",
        "- Return ONLY raw Cypher (no markdown, no explanation, no code fences).\n",
        "- Use LIMIT 10 unless the user asks for more.\n",
        "- Use meaningful aliases in RETURN clauses.\n",
        "\"\"\"\n",
        "\n",
        "\n",
        "def cypher_rag_answer(question: str) -> str:\n",
        "    \"\"\"Let the LLM generate a Cypher query, run it, then summarize results.\"\"\"\n",
        "\n",
        "    # Step 1: Generate Cypher\n",
        "    cypher_response = client.models.generate_content(\n",
        "        model=LLM_MODEL,\n",
        "        contents=question,\n",
        "        config=genai.types.GenerateContentConfig(\n",
        "            system_instruction=CYPHER_SYSTEM_PROMPT,\n",
        "            temperature=0.0,\n",
        "        ),\n",
        "    )\n",
        "    cypher_query = cypher_response.text.strip()\n",
        "    print(f\"ğŸ”§ Generated Cypher:\\n{cypher_query}\\n\")\n",
        "\n",
        "    # Step 2: Execute the Cypher query\n",
        "    try:\n",
        "        with driver.session(database=\"neo4j\") as session:\n",
        "            result = session.run(cypher_query)\n",
        "            records = [dict(record) for record in result]\n",
        "    except Exception as e:\n",
        "        return f\"âŒ Cypher execution error: {e}\"\n",
        "\n",
        "    if not records:\n",
        "        return \"No results found.\"\n",
        "\n",
        "    # Step 3: Summarize with LLM\n",
        "    import json\n",
        "    data_str = json.dumps(records, indent=2, default=str)\n",
        "\n",
        "    summary_response = client.models.generate_content(\n",
        "        model=LLM_MODEL,\n",
        "        contents=f\"Question: {question}\\n\\nQuery results:\\n{data_str}\",\n",
        "        config=genai.types.GenerateContentConfig(\n",
        "            system_instruction=\"Summarize the following database query results in a clear, human-readable way. Use bullet points or a table if appropriate.\",\n",
        "            temperature=0.2,\n",
        "        ),\n",
        "    )\n",
        "    return summary_response.text\n",
        "\n",
        "\n",
        "print(\"âœ… Cypher-Augmented Generation ready.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9220b7b1",
      "metadata": {
        "id": "9220b7b1"
      },
      "outputs": [],
      "source": [
        "# â”€â”€ Analytical questions that benefit from Cypher â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "analytical_questions = [\n",
        "    \"Which user has the most followers and what do they tweet about?\",\n",
        "    # Uncomment to run more questions (watch your rate limits!)\n",
        "    # \"How many tweets were posted from each city?\",\n",
        "    # \"Which hashtags are most frequently used together?\",\n",
        "]\n",
        "\n",
        "for q in analytical_questions:\n",
        "    print(\"=\" * 80)\n",
        "    print(f\"â“ {q}\")\n",
        "    print(\"=\" * 80)\n",
        "    try:\n",
        "\n",
        "        print(cypher_rag_answer(q))        time.sleep(90)\n",
        "\n",
        "    except Exception as e:        print(\"... (sleeping 90s before next query) ...\")\n",
        "\n",
        "        print(f\"âš ï¸ Error: {e}\")    if analytical_questions.index(q) < len(analytical_questions) - 1:\n",
        "\n",
        "        if \"quota\" in str(e).lower() or \"429\" in str(e):    # Add delay between Cypher queries\n",
        "\n",
        "            print(\"â³ Rate limit hit. Please wait before running more queries.\")    print()\n",
        "            break"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "aaf0bbaf",
      "metadata": {
        "id": "aaf0bbaf"
      },
      "source": [
        "---\n",
        "## ğŸŒ Step 11: Geospatial Queries â€” Find Tweets Near a Location\n",
        "\n",
        "Every Tweet and Place node in our graph has a `point()` property storing longitude/latitude. Neo4j's built-in `point.distance()` function lets us find tweets **within a radius** of any coordinate â€” no external GIS tools needed.\n",
        "\n",
        "**Use cases:**\n",
        "- \"What are people tweeting about near Times Square?\"\n",
        "- \"Which users are active within 50 km of Tokyo?\"\n",
        "- \"What topics trend in a geographic cluster?\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c61d32de",
      "metadata": {
        "id": "c61d32de"
      },
      "outputs": [],
      "source": [
        "def find_tweets_near(city_name: str, radius_km: int = 50, limit: int = 10) -> list[dict]:\n",
        "    \"\"\"\n",
        "    Find tweets posted within `radius_km` of a city center.\n",
        "    Uses Neo4j's point.distance() for server-side geospatial filtering.\n",
        "    \"\"\"\n",
        "    cypher = \"\"\"\n",
        "        // Get the city center point\n",
        "        MATCH (p:Place {name: $city})\n",
        "        WITH p.location AS center\n",
        "\n",
        "        // Find tweets within radius\n",
        "        MATCH (u:User)-[:POSTED]->(t:Tweet)-[:LOCATED_AT]->(place:Place)\n",
        "        WHERE point.distance(t.location, center) < $radius_m\n",
        "        OPTIONAL MATCH (t)-[:TAGGED_WITH]->(h:Hashtag)\n",
        "\n",
        "        RETURN t.text                          AS text,\n",
        "               u.username                      AS author,\n",
        "               place.name                      AS place,\n",
        "               collect(DISTINCT h.name)        AS hashtags,\n",
        "               t.likes                         AS likes,\n",
        "               round(point.distance(t.location, center) / 1000.0, 2) AS distance_km\n",
        "        ORDER BY distance_km ASC\n",
        "        LIMIT $limit\n",
        "    \"\"\"\n",
        "    with driver.session(database=\"neo4j\") as session:\n",
        "        result = session.run(cypher, city=city_name, radius_m=radius_km * 1000, limit=limit)\n",
        "        return [dict(record) for record in result]\n",
        "\n",
        "\n",
        "# â”€â”€ Demo: find tweets near multiple cities â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "geo_demos = [\n",
        "    (\"San Francisco, CA\", 50),\n",
        "    (\"Tokyo, JP\", 30),\n",
        "]\n",
        "\n",
        "for city, radius in geo_demos:\n",
        "    results = find_tweets_near(city, radius_km=radius)\n",
        "    print(f\"ğŸ“ Tweets near {city} (within {radius} km):\\n\")\n",
        "    for i, r in enumerate(results, 1):\n",
        "        print(f\"  {i}. @{r['author']} â€” {r['distance_km']} km away\")\n",
        "        print(f\"     \\\"{r['text'][:80]}...\\\"\")\n",
        "        print(f\"     ğŸ·ï¸ {r['hashtags']}  â¤ï¸ {r['likes']} likes\\n\")\n",
        "    print(\"-\" * 60)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ea56013c",
      "metadata": {
        "id": "ea56013c"
      },
      "source": [
        "### ğŸ—ºï¸ Which cities are nearest to each other?\n",
        "\n",
        "We can also compute **inter-city distances** directly in the graph to see how our Place nodes relate geographically."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "907c41a7",
      "metadata": {
        "id": "907c41a7"
      },
      "outputs": [],
      "source": [
        "# â”€â”€ Compute pairwise distances between all cities â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "with driver.session(database=\"neo4j\") as session:\n",
        "    result = session.run(\"\"\"\n",
        "        MATCH (a:Place), (b:Place)\n",
        "        WHERE a.name < b.name  // avoid duplicates\n",
        "        RETURN a.name AS city_a,\n",
        "               b.name AS city_b,\n",
        "               round(point.distance(a.location, b.location) / 1000.0, 0) AS distance_km\n",
        "        ORDER BY distance_km ASC\n",
        "    \"\"\")\n",
        "    print(\"â”€â”€ City-to-City Distances â”€â”€\\n\")\n",
        "    for record in result:\n",
        "        print(f\"  {record['city_a']:>20s}  â†”  {record['city_b']:<20s}  {record['distance_km']:>8.0f} km\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "16e027c0",
      "metadata": {
        "id": "16e027c0"
      },
      "source": [
        "### ğŸ§  Geo-Augmented GraphRAG\n",
        "\n",
        "Combine **geospatial filtering + vector search + graph traversal** into a single retrieval function. This answers questions like:\n",
        "> *\"What are people near London saying about AI?\"*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "afbabe48",
      "metadata": {
        "id": "afbabe48"
      },
      "outputs": [],
      "source": [
        "def geo_graph_rag_search(question: str, city_name: str, radius_km: int = 100, k: int = 5) -> list[dict]:\n",
        "    \"\"\"\n",
        "    Geo-filtered GraphRAG: vector search restricted to tweets\n",
        "    within `radius_km` of a city, then enriched via graph traversal.\n",
        "    \"\"\"\n",
        "    q_embedding = get_embeddings([question])[0]\n",
        "\n",
        "    cypher = \"\"\"\n",
        "        // Get city center\n",
        "        MATCH (p:Place {name: $city})\n",
        "        WITH p.location AS center\n",
        "\n",
        "        // Vector search â€” find semantically similar tweets\n",
        "        CALL db.index.vector.queryNodes($index, $k_broad, $embedding)\n",
        "        YIELD node AS tweet, score\n",
        "\n",
        "        // Geo filter â€” keep only tweets within radius\n",
        "        WHERE point.distance(tweet.location, center) < $radius_m\n",
        "\n",
        "        // Graph traversal â€” enrich\n",
        "        MATCH (author:User)-[:POSTED]->(tweet)\n",
        "        OPTIONAL MATCH (tweet)-[:LOCATED_AT]->(place:Place)\n",
        "        OPTIONAL MATCH (tweet)-[:TAGGED_WITH]->(hashtag:Hashtag)\n",
        "\n",
        "        RETURN tweet.text                       AS text,\n",
        "               score,\n",
        "               author.username                  AS author,\n",
        "               author.followers                 AS author_followers,\n",
        "               place.name                       AS location,\n",
        "               place.country                    AS country,\n",
        "               collect(DISTINCT hashtag.name)   AS hashtags,\n",
        "               tweet.likes                      AS likes,\n",
        "               tweet.retweets                   AS retweets,\n",
        "               round(point.distance(tweet.location, center) / 1000.0, 2) AS distance_km\n",
        "        ORDER BY score DESC\n",
        "        LIMIT $k\n",
        "    \"\"\"\n",
        "    with driver.session(database=\"neo4j\") as session:\n",
        "        # Search a broader pool then geo-filter down\n",
        "        result = session.run(\n",
        "            cypher,\n",
        "            city=city_name,\n",
        "            radius_m=radius_km * 1000,\n",
        "            index=INDEX_NAME,\n",
        "            k_broad=k * 10,  # cast a wider net for vector search\n",
        "            k=k,\n",
        "            embedding=q_embedding,\n",
        "        )\n",
        "        return [dict(record) for record in result]\n",
        "\n",
        "\n",
        "def geo_graph_rag_answer(question: str, city_name: str, radius_km: int = 100, k: int = 5) -> str:\n",
        "    \"\"\"Geo-filtered GraphRAG + Gemini LLM.\"\"\"\n",
        "    results = geo_graph_rag_search(question, city_name, radius_km, k)\n",
        "    if not results:\n",
        "        return f\"No tweets found near {city_name} matching your question.\"\n",
        "\n",
        "    context_lines = []\n",
        "    for i, r in enumerate(results, 1):\n",
        "        context_lines.append(\n",
        "            f\"Tweet {i} (similarity={r['score']:.3f}, {r['distance_km']} km from {city_name}):\\n\"\n",
        "            f\"  Text: \\\"{r['text']}\\\"\\n\"\n",
        "            f\"  Author: @{r['author']} ({r['author_followers']} followers)\\n\"\n",
        "            f\"  Location: {r['location']}, {r['country']}\\n\"\n",
        "            f\"  Hashtags: {', '.join(r['hashtags'])}\\n\"\n",
        "            f\"  Engagement: {r['likes']} likes, {r['retweets']} retweets\"\n",
        "        )\n",
        "    context = \"\\n\\n\".join(context_lines)\n",
        "\n",
        "    response = client.models.generate_content(\n",
        "        model=LLM_MODEL,\n",
        "        contents=f\"Context:\\n{context}\\n\\nQuestion: {question}\",\n",
        "        config=genai.types.GenerateContentConfig(\n",
        "            system_instruction=SYSTEM_PROMPT,\n",
        "            temperature=0.2,\n",
        "        ),\n",
        "    )\n",
        "    return response.text\n",
        "\n",
        "\n",
        "print(\"âœ… Geo-augmented GraphRAG ready.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "efa6b211",
      "metadata": {
        "id": "efa6b211"
      },
      "outputs": [],
      "source": [
        "# â”€â”€ Geo-scoped questions across multiple cities â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "geo_rag_demos = [\n",
        "    (\"What are people saying about AI and machine learning?\", \"London, UK\", 100),\n",
        "    # Uncomment one at a time to avoid rate limits on free tier\n",
        "    # (\"What do people think about cloud computing and serverless?\", \"New York, NY\", 80),\n",
        "    # (\"What do people think about cloud computing and serverless?\", \"San Francisco, CA\", 80),\n",
        "    # (\"What do people think about cloud computing and serverless?\", \"Tokyo, JP\", 80),\n",
        "]\n",
        "\n",
        "for question, city, radius in geo_rag_demos:\n",
        "    print(\"=\" * 70)\n",
        "    print(f\"â“ {question}\")\n",
        "    print(f\"ğŸ“ Within {radius} km of {city}\")\n",
        "    print(\"=\" * 70)\n",
        "    try:\n",
        "\n",
        "        print(geo_graph_rag_answer(question, city, radius_km=radius))        time.sleep(90)\n",
        "\n",
        "    except Exception as e:        print(\"... (sleeping 90s before next geo query) ...\")\n",
        "\n",
        "        print(f\"âš ï¸ Error: {e}\")    if geo_rag_demos.index((question, city, radius)) < len(geo_rag_demos) - 1:\n",
        "\n",
        "        if \"quota\" in str(e).lower() or \"429\" in str(e):    # Add delay between geo queries\n",
        "\n",
        "            print(\"â³ Rate limit hit. Please wait a few minutes before running more queries.\")    print()\n",
        "            break"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "416982ac",
      "metadata": {
        "id": "416982ac"
      },
      "source": [
        "---\n",
        "## ğŸ§¹ Cleanup\n",
        "\n",
        "Close the Neo4j driver when finished."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "be8aed44",
      "metadata": {
        "id": "be8aed44"
      },
      "outputs": [],
      "source": [
        "driver.close()\n",
        "print(\"âœ… Neo4j connection closed.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "25c5e48a",
      "metadata": {
        "id": "25c5e48a"
      },
      "source": [
        "---\n",
        "## ğŸ Conclusion\n",
        "\n",
        "We built a complete **GraphRAG** pipeline from the social media knowledge graph:\n",
        "\n",
        "| Component | What it does |\n",
        "|-----------|-------------|\n",
        "| **Vector Embeddings** | Encode tweet text into 768-d vectors via Gemini |\n",
        "| **Neo4j Vector Index** | Fast cosine-similarity search over tweet embeddings |\n",
        "| **Graph Traversal** | Enrich results with author, location, hashtags, and related tweets |\n",
        "| **LLM Generation** | Produce grounded answers from the enriched context |\n",
        "| **Cypher Generation** | Let the LLM write graph queries for analytical questions |\n",
        "| **Geospatial Queries** | Filter tweets by proximity using `point.distance()` |\n",
        "| **Geo-Augmented GraphRAG** | Combine vector search + geo-filter + graph traversal |\n",
        "\n",
        "**Key Insight:** GraphRAG produces *richer, more accurate* answers than plain vector search because it leverages the **structural relationships** between entities â€” not just text similarity. Adding geospatial filtering lets you scope answers to a specific region.\n",
        "\n",
        "**Next Steps:**\n",
        "- Add **community detection** to find clusters of related users.\n",
        "- Implement **hybrid search** (keyword + vector + graph).\n",
        "- Build a **Streamlit app** for interactive graph exploration.\n",
        "- Add **temporal analysis** â€” how do topics evolve over time?"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "include_colab_link": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
