{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/lbsocial/data-analysis-with-generative-ai/blob/main/GraphRAG_Social_Media_Neo4j.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd5a95af",
   "metadata": {
    "id": "dd5a95af"
   },
   "source": [
    "# ğŸ§  GraphRAG: Retrieval-Augmented Generation with Neo4j Knowledge Graph\n",
    "\n",
    "**Ask natural-language questions â€” get graph-powered answers.**\n",
    "\n",
    "> ğŸ’° **Note:** This notebook is optimized for **paid Gemini API** with rate-limiting delays (20s between operations). Question counts are balanced for tutorial purposes (2 per step) to manage API costs while demonstrating all key features.\n",
    "\n",
    "In the previous notebook (`Social_Media_ETL_Neo4j_Python.ipynb`) we built a social media knowledge graph in Neo4j with **Users, Tweets, Hashtags, and Places**. Now we will layer **AI** on top of that graph to create a **GraphRAG** pipeline.\n",
    "\n",
    "**What is GraphRAG?**\n",
    "\n",
    "Traditional RAG retrieves relevant text chunks from a vector store. **GraphRAG** goes further â€” it combines:\n",
    "1. **Vector Search** â€” find semantically similar tweets using embeddings.\n",
    "2. **Graph Traversal** â€” follow relationships (who posted it, where, which hashtags) to enrich context.\n",
    "3. **LLM Generation** â€” pass the enriched context to a language model for a grounded answer.\n",
    "\n",
    "![GraphRAG Pipeline](https://github.com/lbsocial/data-analysis-with-generative-ai/blob/1660b372f63accb8e55ea4b439924ba764639182/image/Gemini_Generated_Image_vslfdxvslfdxvslf.png?raw=true)\n",
    "\n",
    "**What we will build:**\n",
    "| Step | Action |\n",
    "|------|--------|\n",
    "| 1 | Install dependencies |\n",
    "| 2 | Connect to Neo4j & Gemini |\n",
    "| 3 | Create vector embeddings for every Tweet |\n",
    "| 4 | Build a Neo4j Vector Index |\n",
    "| 5 | Implement pure Vector Search retrieval |\n",
    "| 6 | Implement GraphRAG retrieval (Vector + Graph Traversal) |\n",
    "| 7 | Build the full RAG pipeline with Gemini |\n",
    "| 8 | Compare Vector-only vs GraphRAG answers |\n",
    "| 9 | Interactive GraphRAG query |\n",
    "| 10 | Advanced â€” Cypher-Augmented Generation |\n",
    "| 11 | Geospatial queries & Geo-Augmented GraphRAG |\n",
    "\n",
    "\n",
    "> **Why no LangChain?** Frameworks like LangChain provide convenient abstractions (`GraphCypherQAChain`, `Neo4jVector`, etc.) that can simplify production code. However, in this tutorial we use the **Neo4j Python driver** and **Google GenAI SDK** directly so you can see exactly how each piece works â€” embedding, vector search, graph traversal, prompt formatting, and LLM generation. Once you understand these building blocks, adopting a framework becomes much easier."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46192788",
   "metadata": {
    "id": "46192788"
   },
   "source": [
    "## ğŸ› ï¸ Step 1: Install Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "960268cf",
   "metadata": {
    "id": "960268cf"
   },
   "outputs": [],
   "source": [
    "pip install neo4j google-genai python-dotenv -q"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2316f9be",
   "metadata": {
    "id": "2316f9be"
   },
   "source": [
    "## ğŸ”Œ Step 2: Connect to Neo4j & Gemini\n",
    "\n",
    "We need two connections:\n",
    "- **Neo4j** â€” our knowledge graph.\n",
    "- **Google Gemini** â€” for generating embeddings and LLM answers.\n",
    "\n",
    "> **Note:** This notebook uses Colab Secrets or Google Drive .env file for credentials (see options below)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fa241c5",
   "metadata": {},
   "source": [
    "**ğŸ’¡ Credential Setup (2 Options):**\n",
    "\n",
    "The code below will automatically try these methods **in order** until credentials are found:\n",
    "\n",
    "### Option 1: Colab Secrets (Recommended) ğŸ”‘\n",
    "1. Click the **ğŸ”‘ key icon** in the left sidebar (Secrets panel)\n",
    "2. Add three secrets with these exact names:\n",
    "   - `NEO4J_URI`\n",
    "   - `NEO4J_PASSWORD`\n",
    "   - `GOOGLE_API_KEY`\n",
    "3. Enable **notebook access** for each secret (toggle switch)\n",
    "4. Run Step 2 - credentials load automatically\n",
    "\n",
    "### Option 2: Google Drive .env File â˜ï¸\n",
    "1. Create a `.env` file with your credentials (see `.env.example`)\n",
    "2. Upload it to **Google Drive â†’ Colab Notebooks** folder\n",
    "   - Path: `MyDrive/Colab Notebooks/.env`\n",
    "3. Run Step 2 - it will mount Drive and load automatically\n",
    "\n",
    "> **Note:** This notebook is designed for Google Colab. If you want to run it locally with Jupyter, you'll need to adapt the credential loading code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cec4559",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "4cec4559",
    "outputId": "93dfaba0-e8b9-4062-f9d6-bdcf3329f896"
   },
   "outputs": [],
   "source": [
    "from neo4j import GraphDatabase\n",
    "from google import genai\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "NEO4J_URI = None\n",
    "NEO4J_PASSWORD = None\n",
    "GOOGLE_API_KEY = None\n",
    "\n",
    "# â”€â”€ Method 1: Try Colab Secrets first â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "try:\n",
    "    from google.colab import userdata\n",
    "    NEO4J_URI = userdata.get('NEO4J_URI')\n",
    "    NEO4J_PASSWORD = userdata.get('NEO4J_PASSWORD')\n",
    "    GOOGLE_API_KEY = userdata.get('GOOGLE_API_KEY')\n",
    "    if all([NEO4J_URI, NEO4J_PASSWORD, GOOGLE_API_KEY]):\n",
    "        print(\"âœ… Loaded credentials from Colab Secrets\")\n",
    "except:\n",
    "    pass\n",
    "\n",
    "# â”€â”€ Method 2: Try .env file from Google Drive â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "if not all([NEO4J_URI, NEO4J_PASSWORD, GOOGLE_API_KEY]):\n",
    "    try:\n",
    "        # Mount Google Drive\n",
    "        from google.colab import drive\n",
    "        drive.mount('/content/drive', force_remount=False)\n",
    "        print(\"ğŸ“‚ Google Drive mounted\")\n",
    "        \n",
    "        # Try loading from .env in Colab Notebooks folder\n",
    "        from dotenv import load_dotenv\n",
    "        drive_env_paths = [\n",
    "            Path('/content/drive/MyDrive/Colab Notebooks/.env'),\n",
    "            Path('/content/drive/MyDrive/.env'),\n",
    "        ]\n",
    "        \n",
    "        for env_path in drive_env_paths:\n",
    "            if env_path.exists():\n",
    "                load_dotenv(env_path)\n",
    "                NEO4J_URI = os.getenv('NEO4J_URI')\n",
    "                NEO4J_PASSWORD = os.getenv('NEO4J_PASSWORD')\n",
    "                GOOGLE_API_KEY = os.getenv('GOOGLE_API_KEY')\n",
    "                if all([NEO4J_URI, NEO4J_PASSWORD, GOOGLE_API_KEY]):\n",
    "                    print(f\"âœ… Loaded credentials from Google Drive: {env_path}\")\n",
    "                    break\n",
    "    except Exception as e:\n",
    "        print(f\"âš ï¸ Could not load from Google Drive: {e}\")\n",
    "\n",
    "# â”€â”€ Show credential status â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "print(f\"\\nğŸ” Credential status:\")\n",
    "print(f\"  NEO4J_URI: {'âœ“' if NEO4J_URI else 'âœ—'}\")\n",
    "print(f\"  NEO4J_PASSWORD: {'âœ“' if NEO4J_PASSWORD else 'âœ—'}\")\n",
    "print(f\"  GOOGLE_API_KEY: {'âœ“' if GOOGLE_API_KEY else 'âœ—'}\")\n",
    "\n",
    "# â”€â”€ Validate credentials â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "if not all([NEO4J_URI, NEO4J_PASSWORD, GOOGLE_API_KEY]):\n",
    "    print(\"\\nâŒ MISSING CREDENTIALS!\")\n",
    "    print(\"\\nğŸ“‹ Choose one of these options:\")\n",
    "    print(\"\\n  Option 1: Colab Secrets (Recommended)\")\n",
    "    print(\"    1. Click the ğŸ”‘ key icon in the left sidebar\")\n",
    "    print(\"    2. Add secrets: NEO4J_URI, NEO4J_PASSWORD, GOOGLE_API_KEY\")\n",
    "    print(\"    3. Enable notebook access for each\")\n",
    "    print(\"    4. Re-run this cell\")\n",
    "    print(\"\\n  Option 2: Google Drive .env File\")\n",
    "    print(\"    1. Create .env file with your credentials\")\n",
    "    print(\"    2. Upload to: Google Drive â†’ Colab Notebooks/.env\")\n",
    "    print(\"    3. Re-run this cell (will mount Drive automatically)\")\n",
    "    raise ValueError(\"Missing credentials! Choose Option 1 or Option 2 above.\")\n",
    "\n",
    "NEO4J_AUTH = (\"neo4j\", NEO4J_PASSWORD)\n",
    "\n",
    "# â”€â”€ Initialize Gemini Client â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "client = genai.Client(api_key=GOOGLE_API_KEY)\n",
    "\n",
    "# â”€â”€ Verify Neo4j Connection â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "driver = GraphDatabase.driver(NEO4J_URI, auth=NEO4J_AUTH)\n",
    "driver.verify_connectivity()\n",
    "print(\"âœ… Connected to Neo4j\")\n",
    "\n",
    "# â”€â”€ Verify Gemini Connection â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "test = client.models.get(model=\"gemini-2.0-flash\")\n",
    "print(f\"âœ… Connected to Gemini ({test.display_name})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cca90c6f",
   "metadata": {
    "id": "cca90c6f"
   },
   "source": [
    "## ğŸ“Š Step 2b: Verify the Graph Data\n",
    "\n",
    "Let's confirm the graph we built in the ETL notebook is ready."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97025886",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "97025886",
    "outputId": "8ad28d7f-8b09-4fa9-d5ea-645574b8c1a9"
   },
   "outputs": [],
   "source": [
    "# Quick health check â€” count nodes by label\n",
    "with driver.session(database=\"neo4j\") as session:\n",
    "    result = session.run(\"\"\"\n",
    "        CALL {\n",
    "            MATCH (t:Tweet) RETURN 'Tweet' AS label, count(t) AS count\n",
    "            UNION ALL\n",
    "            MATCH (u:User)  RETURN 'User'  AS label, count(u) AS count\n",
    "            UNION ALL\n",
    "            MATCH (h:Hashtag) RETURN 'Hashtag' AS label, count(h) AS count\n",
    "            UNION ALL\n",
    "            MATCH (p:Place) RETURN 'Place' AS label, count(p) AS count\n",
    "        }\n",
    "        RETURN label, count\n",
    "    \"\"\")\n",
    "    print(\"â”€â”€ Graph Node Summary â”€â”€\")\n",
    "    for record in result:\n",
    "        print(f\"  {record['label']:>10s}: {record['count']}\")\n",
    "\n",
    "# Sample one tweet with all its relationships\n",
    "with driver.session(database=\"neo4j\") as session:\n",
    "    result = session.run(\"\"\"\n",
    "        MATCH (u:User)-[:POSTED]->(t:Tweet)-[:LOCATED_AT]->(p:Place)\n",
    "        OPTIONAL MATCH (t)-[:TAGGED_WITH]->(h:Hashtag)\n",
    "        RETURN u.username AS user, t.text AS text,\n",
    "               p.name AS place, collect(h.name) AS hashtags\n",
    "        LIMIT 3\n",
    "    \"\"\")\n",
    "    print(\"\\nâ”€â”€ Sample Tweets â”€â”€\")\n",
    "    for record in result:\n",
    "        print(f\"  @{record['user']} from {record['place']}\")\n",
    "        print(f\"    \\\"{record['text'][:80]}...\\\"\")\n",
    "        print(f\"    Tags: {record['hashtags']}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7d9a78d",
   "metadata": {
    "id": "c7d9a78d"
   },
   "source": [
    "---\n",
    "## ğŸ§¬ Step 3: Generate Vector Embeddings for Tweets\n",
    "\n",
    "We use Gemini's `gemini-embedding-001` model to convert every tweet's text into a **3072-dimensional vector**. This vector captures the *semantic meaning* of the text.\n",
    "\n",
    "We then write each embedding back to the `Tweet` node as a property called `embedding`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d545a106",
   "metadata": {
    "id": "d545a106"
   },
   "outputs": [],
   "source": [
    "EMBEDDING_MODEL = \"models/gemini-embedding-001\"\n",
    "EMBEDDING_DIM = 3072\n",
    "\n",
    "\n",
    "def get_embeddings(texts: list[str]) -> list[list[float]]:\n",
    "    \"\"\"Get Gemini embeddings for a batch of texts.\"\"\"\n",
    "    response = client.models.embed_content(\n",
    "        model=EMBEDDING_MODEL,\n",
    "        contents=texts,\n",
    "    )\n",
    "    return [emb.values for emb in response.embeddings]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "M8lPYW_b2yX8",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "M8lPYW_b2yX8",
    "outputId": "863e1bfe-ebf5-4f1a-e291-7055e4902ce3"
   },
   "outputs": [],
   "source": [
    "# â”€â”€ Check if embeddings already exist â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "with driver.session(database=\"neo4j\") as session:\n",
    "    result = session.run(\"\"\"\n",
    "        MATCH (t:Tweet)\n",
    "        RETURN count(t) AS total_tweets,\n",
    "               count(t.embedding) AS tweets_with_embeddings\n",
    "    \"\"\")\n",
    "    record = result.single()\n",
    "    total_tweets = record[\"total_tweets\"]\n",
    "    tweets_with_embeddings = record[\"tweets_with_embeddings\"]\n",
    "\n",
    "print(f\"ğŸ“Š Tweet embedding status:\")\n",
    "print(f\"   Total tweets: {total_tweets}\")\n",
    "print(f\"   Tweets with embeddings: {tweets_with_embeddings}\")\n",
    "\n",
    "# â”€â”€ Only embed if needed â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "if tweets_with_embeddings == total_tweets and total_tweets > 0:\n",
    "    print(\"\\nâœ… All tweets already have embeddings! Skipping embedding step.\")\n",
    "    print(\"   ğŸ’¡ This saves significant API quota!\")\n",
    "else:\n",
    "    print(f\"\\nâ³ Need to embed {total_tweets - tweets_with_embeddings} tweets...\")\n",
    "\n",
    "    # Fetch tweets that need embeddings\n",
    "    with driver.session(database=\"neo4j\") as session:\n",
    "        result = session.run(\"\"\"\n",
    "            MATCH (t:Tweet)\n",
    "            WHERE t.embedding IS NULL\n",
    "            RETURN t.id AS id, t.text AS text\n",
    "        \"\"\")\n",
    "        tweets = [(record[\"id\"], record[\"text\"]) for record in result]\n",
    "\n",
    "    if len(tweets) == 0:\n",
    "        print(\"âœ… No tweets need embedding!\")\n",
    "    else:\n",
    "        print(f\"ğŸ“„ Embedding {len(tweets)} tweets...\")\n",
    "\n",
    "        # â”€â”€ Batch embed (20 tweets at a time) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "        BATCH_SIZE = 20\n",
    "        for i in range(0, len(tweets), BATCH_SIZE):\n",
    "            batch = tweets[i : i + BATCH_SIZE]\n",
    "            ids = [t[0] for t in batch]\n",
    "            texts = [t[1] for t in batch]\n",
    "\n",
    "            embeddings = get_embeddings(texts)\n",
    "\n",
    "            # Write embeddings back to Neo4j\n",
    "            with driver.session(database=\"neo4j\") as session:\n",
    "                session.run(\n",
    "                    \"\"\"\n",
    "                    UNWIND $data AS row\n",
    "                    MATCH (t:Tweet {id: row.id})\n",
    "                    SET t.embedding = row.embedding\n",
    "                    \"\"\",\n",
    "                    data=[{\"id\": id_, \"embedding\": emb} for id_, emb in zip(ids, embeddings)],\n",
    "                )\n",
    "\n",
    "            print(f\"  âœ… Embedded batch {i // BATCH_SIZE + 1}/{(len(tweets) - 1) // BATCH_SIZE + 1}\")\n",
    "\n",
    "        print(\"\\nğŸ‰ All tweet embeddings stored in Neo4j!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47fc570a",
   "metadata": {
    "id": "47fc570a"
   },
   "source": [
    "---\n",
    "## ğŸ—‚ï¸ Step 4: Create a Neo4j Vector Index\n",
    "\n",
    "A **vector index** lets Neo4j perform fast approximate-nearest-neighbor (ANN) search on the embedding property. This is the foundation of our retrieval step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25bb5bb8",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "25bb5bb8",
    "outputId": "d284e454-ca76-4a0b-cd1e-769a215e4a47"
   },
   "outputs": [],
   "source": [
    "INDEX_NAME = \"tweet_embeddings\"\n",
    "\n",
    "with driver.session(database=\"neo4j\") as session:\n",
    "    # Ensure the index is dropped idempotently\n",
    "    print(f\"Attempting to drop existing index '{INDEX_NAME}' if it exists...\")\n",
    "    session.run(f\"DROP INDEX {INDEX_NAME} IF EXISTS\")\n",
    "    print(f\"âœ… Index '{INDEX_NAME}' dropped (if it existed).\")\n",
    "\n",
    "    # Create the vector index with the correct dimensions\n",
    "    session.run(f\"\"\"\n",
    "        CREATE VECTOR INDEX {INDEX_NAME}\n",
    "        FOR (t:Tweet)\n",
    "        ON (t.embedding)\n",
    "        OPTIONS {{\n",
    "            indexConfig: {{\n",
    "                `vector.dimensions`: {EMBEDDING_DIM},\n",
    "                `vector.similarity_function`: 'cosine'\n",
    "            }}\n",
    "        }}\n",
    "    \"\"\")\n",
    "    print(f\"âœ… Vector index '{INDEX_NAME}' created with {EMBEDDING_DIM} dimensions.\")\n",
    "\n",
    "# Verify index details, specifically dimensions\n",
    "with driver.session(database=\"neo4j\") as session:\n",
    "    # Run SHOW VECTOR INDEXES and then filter in Python\n",
    "    result = session.run(\"SHOW VECTOR INDEXES\")\n",
    "    found_index = False\n",
    "    for record in result:\n",
    "        if record['name'] == INDEX_NAME:\n",
    "            found_index = True\n",
    "            print(f\"  Index: {record['name']}\")\n",
    "            print(f\"  State: {record['state']}\")\n",
    "            # Extract dimensions from the 'properties' map\n",
    "            if 'properties' in record and 'indexConfig' in record['properties']:\n",
    "                config = record['properties']['indexConfig']\n",
    "                dimensions = config.get('vector.dimensions')\n",
    "                if dimensions:\n",
    "                    print(f\"  Actual Dimensions: {dimensions}\")\n",
    "                else:\n",
    "                    print(\"  Dimensions not found in index properties.\")\n",
    "            else:\n",
    "                print(\"  Index configuration properties not found.\")\n",
    "            break\n",
    "    if not found_index:\n",
    "        print(f\"âŒ Index '{INDEX_NAME}' not found after creation.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fae6b082",
   "metadata": {
    "id": "fae6b082"
   },
   "source": [
    "---\n",
    "## ğŸ” Step 5: Pure Vector Search (Baseline)\n",
    "\n",
    "First, let's implement a plain **vector similarity search**. We embed the user's question, then find the `k` most similar tweets. This is what a traditional RAG system does â€” **no graph knowledge** is used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd877d2a",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "cd877d2a",
    "outputId": "af75b66b-251e-44e0-9559-563a42452a5a"
   },
   "outputs": [],
   "source": [
    "def vector_search(question: str, k: int = 5) -> list[dict]:\n",
    "    \"\"\"\n",
    "    Plain vector search â€” returns the top-k most similar tweets.\n",
    "    No graph traversal.\n",
    "    \"\"\"\n",
    "    q_embedding = get_embeddings([question])[0]\n",
    "\n",
    "    cypher = \"\"\"\n",
    "        CALL db.index.vector.queryNodes($index, $k, $embedding)\n",
    "        YIELD node AS tweet, score\n",
    "        RETURN tweet.id    AS id,\n",
    "               tweet.text  AS text,\n",
    "               tweet.likes AS likes,\n",
    "               score\n",
    "        ORDER BY score DESC\n",
    "    \"\"\"\n",
    "    with driver.session(database=\"neo4j\") as session:\n",
    "        result = session.run(cypher, index=INDEX_NAME, k=k, embedding=q_embedding)\n",
    "        return [dict(record) for record in result]\n",
    "\n",
    "\n",
    "# â”€â”€ Test it â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "question = \"What are people saying about graph databases?\"\n",
    "results = vector_search(question)\n",
    "\n",
    "print(f\"ğŸ” Vector Search Results for: '{question}'\\n\")\n",
    "for i, r in enumerate(results, 1):\n",
    "    print(f\"  {i}. [score={r['score']:.4f}] {r['text'][:90]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f5b073e",
   "metadata": {
    "id": "7f5b073e"
   },
   "source": [
    "---\n",
    "## ğŸ•¸ï¸ Step 6: GraphRAG Retrieval (Vector + Graph Traversal)\n",
    "\n",
    "This is the key innovation. After finding similar tweets via vector search, we **traverse the graph** to collect rich context:\n",
    "\n",
    "| Traversal | What we get |\n",
    "|-----------|-------------|\n",
    "| `Tweet â† POSTED â† User` | Who wrote it? How many followers? |\n",
    "| `Tweet â†’ LOCATED_AT â†’ Place` | Where was it posted? |\n",
    "| `Tweet â†’ TAGGED_WITH â†’ Hashtag` | What topics is it about? |\n",
    "| `User â†’ POSTED â†’ OtherTweets` | What else has this user said? (context expansion) |\n",
    "| `Hashtag â† TAGGED_WITH â† OtherTweets` | What else is tagged with the same topics? |\n",
    "\n",
    "This gives the LLM **much richer context** than bare text chunks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51b3d6f7",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "51b3d6f7",
    "outputId": "83a96e8f-7c5e-4f30-c398-e79eb53f789f"
   },
   "outputs": [],
   "source": [
    "def graph_rag_search(question: str, k: int = 5) -> list[dict]:\n",
    "    \"\"\"\n",
    "    GraphRAG retrieval â€” vector search + graph traversal.\n",
    "    Returns enriched context for each matching tweet.\n",
    "    \"\"\"\n",
    "    q_embedding = get_embeddings([question])[0]\n",
    "\n",
    "    cypher = \"\"\"\n",
    "        // â”€â”€ 1. Vector Search: find semantically similar tweets â”€â”€\n",
    "        CALL db.index.vector.queryNodes($index, $k, $embedding)\n",
    "        YIELD node AS tweet, score\n",
    "\n",
    "        // â”€â”€ 2. Graph Traversal: enrich with relationships â”€â”€\n",
    "        // Get the author\n",
    "        MATCH (author:User)-[:POSTED]->(tweet)\n",
    "\n",
    "        // Get the location\n",
    "        OPTIONAL MATCH (tweet)-[:LOCATED_AT]->(place:Place)\n",
    "\n",
    "        // Get all hashtags on this tweet\n",
    "        OPTIONAL MATCH (tweet)-[:TAGGED_WITH]->(hashtag:Hashtag)\n",
    "\n",
    "        // â”€â”€ 3. Context Expansion: other tweets by same author â”€â”€\n",
    "        OPTIONAL MATCH (author)-[:POSTED]->(other_tweet:Tweet)\n",
    "        WHERE other_tweet.id <> tweet.id\n",
    "\n",
    "        // â”€â”€ 4. Context Expansion: co-occurring tweets via hashtags â”€â”€\n",
    "        OPTIONAL MATCH (hashtag)<-[:TAGGED_WITH]-(related_tweet:Tweet)\n",
    "        WHERE related_tweet.id <> tweet.id\n",
    "\n",
    "        RETURN tweet.id                        AS tweet_id,\n",
    "               tweet.text                      AS text,\n",
    "               tweet.likes                     AS likes,\n",
    "               tweet.retweets                  AS retweets,\n",
    "               score,\n",
    "\n",
    "               // Author context\n",
    "               author.username                 AS author,\n",
    "               author.followers                AS author_followers,\n",
    "\n",
    "               // Location context\n",
    "               place.name                      AS location,\n",
    "               place.country                   AS country,\n",
    "\n",
    "               // Hashtag context\n",
    "               collect(DISTINCT hashtag.name)   AS hashtags,\n",
    "\n",
    "               // Expanded context\n",
    "               collect(DISTINCT other_tweet.text)[0..3]   AS author_other_tweets,\n",
    "               collect(DISTINCT related_tweet.text)[0..3] AS related_tweets_via_hashtag\n",
    "\n",
    "        ORDER BY score DESC\n",
    "    \"\"\"\n",
    "    with driver.session(database=\"neo4j\") as session:\n",
    "        result = session.run(cypher, index=INDEX_NAME, k=k, embedding=q_embedding)\n",
    "        return [dict(record) for record in result]\n",
    "\n",
    "\n",
    "# â”€â”€ Test it â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "question = \"What are people saying about graph databases?\"\n",
    "results = graph_rag_search(question)\n",
    "\n",
    "print(f\"ğŸ•¸ï¸ GraphRAG Results for: '{question}'\\n\")\n",
    "for i, r in enumerate(results, 1):\n",
    "    print(f\"  {i}. [score={r['score']:.4f}] @{r['author']} ({r['author_followers']} followers)\")\n",
    "    print(f\"     ğŸ“ {r['location']}, {r['country']}\")\n",
    "    print(f\"     ğŸ’¬ \\\"{r['text'][:80]}...\\\"\")\n",
    "    print(f\"     ğŸ·ï¸  Tags: {r['hashtags']}\")\n",
    "    if r['related_tweets_via_hashtag']:\n",
    "        print(f\"     ğŸ”— Related: \\\"{r['related_tweets_via_hashtag'][0][:60]}...\\\"\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc78870a",
   "metadata": {
    "id": "dc78870a"
   },
   "source": [
    "---\n",
    "## ğŸ¤– Step 7: Build the Full RAG Pipeline with Gemini\n",
    "\n",
    "Now we connect everything: **retrieve** enriched context from the graph, **format** it into a prompt, and **generate** an answer with Gemini.\n",
    "\n",
    "We build two pipelines side-by-side:\n",
    "- `rag_answer()` â€” plain vector retrieval + Gemini\n",
    "- `graph_rag_answer()` â€” GraphRAG retrieval + Gemini"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40af5918",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "40af5918",
    "outputId": "05e074e2-2bef-4cbc-dc91-06b863180bd7"
   },
   "outputs": [],
   "source": [
    "LLM_MODEL = \"gemini-2.0-flash\"\n",
    "\n",
    "SYSTEM_PROMPT = \"\"\"You are a social media analyst assistant.\n",
    "Answer the user's question based ONLY on the retrieved context below.\n",
    "If the context doesn't contain enough information, say so.\n",
    "Always cite specific tweets, users, or locations when relevant.\n",
    "Be concise but thorough.\"\"\"\n",
    "\n",
    "\n",
    "def format_vector_context(results: list[dict]) -> str:\n",
    "    \"\"\"Format plain vector search results into a text block.\"\"\"\n",
    "    lines = []\n",
    "    for i, r in enumerate(results, 1):\n",
    "        lines.append(f\"Tweet {i} (similarity={r['score']:.3f}, likes={r['likes']}):\\n  \\\"{r['text']}\\\"\")\n",
    "    return \"\\n\\n\".join(lines)\n",
    "\n",
    "\n",
    "def format_graph_context(results: list[dict]) -> str:\n",
    "    \"\"\"Format GraphRAG results into a rich context block.\"\"\"\n",
    "    lines = []\n",
    "    for i, r in enumerate(results, 1):\n",
    "        block = f\"\"\"Tweet {i} (similarity={r['score']:.3f}):\n",
    "  Text: \"{r['text']}\"\n",
    "  Author: @{r['author']} ({r['author_followers']} followers)\n",
    "  Location: {r['location']}, {r['country']}\n",
    "  Engagement: {r['likes']} likes, {r['retweets']} retweets\n",
    "  Hashtags: {', '.join(r['hashtags'])}\"\"\"\n",
    "        if r.get('author_other_tweets'):\n",
    "            block += f\"\\n  Other tweets by @{r['author']}:\"\n",
    "            for t in r['author_other_tweets']:\n",
    "                block += f\"\\n    - \\\"{t[:80]}...\\\"\"\n",
    "        if r.get('related_tweets_via_hashtag'):\n",
    "            block += f\"\\n  Related tweets (same hashtags):\"\n",
    "            for t in r['related_tweets_via_hashtag']:\n",
    "                block += f\"\\n    - \\\"{t[:80]}...\\\"\"\n",
    "        lines.append(block)\n",
    "    return \"\\n\\n\".join(lines)\n",
    "\n",
    "\n",
    "def rag_answer(question: str, k: int = 5) -> str:\n",
    "    \"\"\"Traditional RAG: vector search + LLM.\"\"\"\n",
    "    results = vector_search(question, k)\n",
    "    context = format_vector_context(results)\n",
    "\n",
    "    response = client.models.generate_content(\n",
    "        model=LLM_MODEL,\n",
    "        contents=f\"Context:\\n{context}\\n\\nQuestion: {question}\",\n",
    "        config=genai.types.GenerateContentConfig(\n",
    "            system_instruction=SYSTEM_PROMPT,\n",
    "            temperature=0.2,\n",
    "        ),\n",
    "    )\n",
    "    return response.text\n",
    "\n",
    "\n",
    "def graph_rag_answer(question: str, k: int = 5) -> str:\n",
    "    \"\"\"GraphRAG: vector search + graph traversal + LLM.\"\"\"\n",
    "    results = graph_rag_search(question, k)\n",
    "    context = format_graph_context(results)\n",
    "\n",
    "    response = client.models.generate_content(\n",
    "        model=LLM_MODEL,\n",
    "        contents=f\"Context:\\n{context}\\n\\nQuestion: {question}\",\n",
    "        config=genai.types.GenerateContentConfig(\n",
    "            system_instruction=SYSTEM_PROMPT,\n",
    "            temperature=0.2,\n",
    "        ),\n",
    "    )\n",
    "    return response.text\n",
    "\n",
    "\n",
    "print(\"âœ… RAG pipelines ready.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a03fa285",
   "metadata": {
    "id": "a03fa285"
   },
   "source": [
    "---\n",
    "## âš–ï¸ Step 8: Compare Vector-Only RAG vs GraphRAG\n",
    "\n",
    "Let's ask the same questions using both approaches and compare the quality of answers.\n",
    "\n",
    "**What to observe:**\n",
    "- **Traditional RAG**: Returns text chunks based solely on semantic similarity\n",
    "- **GraphRAG**: Returns richer context including author info, locations, hashtags, and related content\n",
    "\n",
    "**Expected difference:** GraphRAG answers should be more detailed and cite specific users, locations, and engagement metrics because it leverages graph relationships, not just text similarity.\n",
    "\n",
    "**Tutorial Note:** We run 2 comparison questions. Each makes 2 API calls (one for traditional RAG, one for GraphRAG)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af9459ed",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 792
    },
    "id": "af9459ed",
    "outputId": "8a85ea7b-6bb4-4dca-9d40-ee29ab12a340"
   },
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "comparison_questions = [\n",
    "    \"What are people saying about graph databases and who are the most active users talking about them?\",\n",
    "    \"Which cities generate the most discussion about AI and cloud computing?\",\n",
    "]\n",
    "\n",
    "for question in comparison_questions:\n",
    "    print(\"=\" * 80)\n",
    "    print(f\"â“ Question: {question}\")\n",
    "    print(\"=\" * 80)\n",
    "\n",
    "    print(\"\\nğŸ“„ â”€â”€ TRADITIONAL RAG (Vector Only) â”€â”€\")\n",
    "    try:\n",
    "        print(rag_answer(question))\n",
    "    except Exception as e:\n",
    "        print(f\"âš ï¸ Error during Vector RAG: {e}\")\n",
    "\n",
    "    print(\"\\nğŸ•¸ï¸ â”€â”€ GRAPHRAG (Vector + Graph Traversal) â”€â”€\")\n",
    "    try:\n",
    "        print(graph_rag_answer(question))\n",
    "    except Exception as e:\n",
    "        print(f\"âš ï¸ Error during GraphRAG: {e}\")\n",
    "\n",
    "    print()  # Blank line between questions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42529791",
   "metadata": {
    "id": "42529791"
   },
   "source": [
    "---\n",
    "## ğŸ§ª Step 9: Interactive GraphRAG Query\n",
    "\n",
    "Now you can test the system with your own questions! \n",
    "\n",
    "**Try modifying the question below to explore:**\n",
    "- Topics in specific cities: *\"What are people discussing in New York?\"*\n",
    "- Technology trends: *\"What do people say about blockchain?\"*\n",
    "- User-specific queries: *\"What topics does @user123 tweet about?\"*\n",
    "\n",
    "The GraphRAG pipeline will automatically retrieve related tweets, users, and hashtags to provide contextual answers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56517c87",
   "metadata": {
    "id": "56517c87"
   },
   "outputs": [],
   "source": [
    "# â”€â”€ Change this question to anything you want â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "your_question = \"What topics are trending in San Francisco?\"\n",
    "\n",
    "print(f\"â“ {your_question}\\n\")\n",
    "print(graph_rag_answer(your_question))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "755b4fcf",
   "metadata": {
    "id": "755b4fcf"
   },
   "source": [
    "---\n",
    "## ğŸ”¬ Step 10: Advanced â€” Cypher-Augmented Generation\n",
    "\n",
    "For structured analytical questions (\"How many tweets per city?\"), we can let the LLM **generate Cypher queries** directly. This combines the power of graph queries with natural language.\n",
    "\n",
    "**How it works:**\n",
    "1. **Generate**: LLM creates a Cypher query based on user's question and graph schema\n",
    "2. **Execute**: Run the generated query against Neo4j\n",
    "3. **Summarize**: LLM formats the raw results into a human-readable answer\n",
    "\n",
    "**Benefits:**\n",
    "- Handles aggregations, counts, and complex analytical queries\n",
    "- No need to manually write Cypher for every question\n",
    "- LLM ensures query matches the actual schema\n",
    "\n",
    "**Tutorial Note:** We demonstrate 2 analytical questions. In production, you can add validation to check generated queries before execution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b90f060",
   "metadata": {
    "id": "3b90f060"
   },
   "outputs": [],
   "source": [
    "CYPHER_SYSTEM_PROMPT = \"\"\"You are a Neo4j Cypher expert. Given the user's question, generate a Cypher query to answer it.\n",
    "\n",
    "The graph schema is:\n",
    "- (:User {id, username, name, followers, following, tweet_count})\n",
    "    -[:POSTED]->(:Tweet {id, text, created_at, likes, retweets, replies, location, embedding})\n",
    "- (:Tweet)-[:LOCATED_AT]->(:Place {name, country, location})\n",
    "- (:Tweet)-[:TAGGED_WITH]->(:Hashtag {name})\n",
    "\n",
    "Rules:\n",
    "- Return ONLY raw Cypher (no markdown, no explanation, no code fences).\n",
    "- Use LIMIT 10 unless the user asks for more.\n",
    "- Use meaningful aliases in RETURN clauses.\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "def cypher_rag_answer(question: str) -> str:\n",
    "    \"\"\"Let the LLM generate a Cypher query, run it, then summarize results.\"\"\"\n",
    "\n",
    "    # Step 1: Generate Cypher\n",
    "    cypher_response = client.models.generate_content(\n",
    "        model=LLM_MODEL,\n",
    "        contents=question,\n",
    "        config=genai.types.GenerateContentConfig(\n",
    "            system_instruction=CYPHER_SYSTEM_PROMPT,\n",
    "            temperature=0.0,\n",
    "        ),\n",
    "    )\n",
    "    cypher_query = cypher_response.text.strip()\n",
    "    \n",
    "    # Strip markdown code fences if present\n",
    "    if cypher_query.startswith('```'):\n",
    "        # Remove ```cypher or ``` at start and ``` at end\n",
    "        lines = cypher_query.split('\\n')\n",
    "        if lines[0].startswith('```'):\n",
    "            lines = lines[1:]  # Remove first line with ```cypher\n",
    "        if lines and lines[-1].strip() == '```':\n",
    "            lines = lines[:-1]  # Remove last line with ```\n",
    "        cypher_query = '\\n'.join(lines).strip()\n",
    "    \n",
    "    print(f\"ğŸ”§ Generated Cypher:\\n{cypher_query}\\n\")\n",
    "\n",
    "    # Step 2: Execute the Cypher query\n",
    "    try:\n",
    "        with driver.session(database=\"neo4j\") as session:\n",
    "            result = session.run(cypher_query)\n",
    "            records = [dict(record) for record in result]\n",
    "    except Exception as e:\n",
    "        return f\"âŒ Cypher execution error: {e}\"\n",
    "\n",
    "    if not records:\n",
    "        return \"No results found.\"\n",
    "\n",
    "    # Step 3: Summarize with LLM\n",
    "    import json\n",
    "    data_str = json.dumps(records, indent=2, default=str)\n",
    "\n",
    "    summary_response = client.models.generate_content(\n",
    "        model=LLM_MODEL,\n",
    "        contents=f\"Question: {question}\\n\\nQuery results:\\n{data_str}\",\n",
    "        config=genai.types.GenerateContentConfig(\n",
    "            system_instruction=\"Summarize the following database query results in a clear, human-readable way. Use bullet points or a table if appropriate.\",\n",
    "            temperature=0.2,\n",
    "        ),\n",
    "    )\n",
    "    return summary_response.text\n",
    "\n",
    "\n",
    "print(\"âœ… Cypher-Augmented Generation ready.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9220b7b1",
   "metadata": {
    "id": "9220b7b1"
   },
   "outputs": [],
   "source": [
    "# â”€â”€ Analytical questions that benefit from Cypher â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "analytical_questions = [\n",
    "    \"Which user has the most followers and what do they tweet about?\",\n",
    "    \"How many tweets were posted from each city?\",\n",
    "]\n",
    "\n",
    "for q in analytical_questions:\n",
    "    print(\"=\" * 80)\n",
    "    print(f\"â“ {q}\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    try:\n",
    "        print(cypher_rag_answer(q))\n",
    "    except Exception as e:\n",
    "        print(f\"âš ï¸ Error: {e}\")\n",
    "    \n",
    "    # Delay to avoid rate limits (each question = 2 API calls)\n",
    "    if analytical_questions.index(q) < len(analytical_questions) - 1:\n",
    "        print(\"\\nâ³ Waiting 20 seconds to avoid rate limits...\")\n",
    "        time.sleep(20)\n",
    "    \n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aaf0bbaf",
   "metadata": {
    "id": "aaf0bbaf"
   },
   "source": [
    "---\n",
    "## ğŸŒ Step 11: Geospatial Queries â€” Find Tweets Near a Location\n",
    "\n",
    "Every Tweet and Place node in our graph has a `point()` property storing longitude/latitude. Neo4j's built-in `point.distance()` function lets us find tweets **within a radius** of any coordinate â€” no external GIS tools needed.\n",
    "\n",
    "**Use cases:**\n",
    "- \"What are people tweeting about near Times Square?\"\n",
    "- \"Which users are active within 50 km of Tokyo?\"\n",
    "- \"What topics trend in a geographic cluster?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c61d32de",
   "metadata": {
    "id": "c61d32de"
   },
   "outputs": [],
   "source": [
    "def find_tweets_near(city_name: str, radius_km: int = 50, limit: int = 10) -> list[dict]:\n",
    "    \"\"\"\n",
    "    Find tweets posted within `radius_km` of a city center.\n",
    "    Uses Neo4j's point.distance() for server-side geospatial filtering.\n",
    "    \"\"\"\n",
    "    cypher = \"\"\"\n",
    "        // Get the city center point\n",
    "        MATCH (p:Place {name: $city})\n",
    "        WITH p.location AS center\n",
    "\n",
    "        // Find tweets within radius\n",
    "        MATCH (u:User)-[:POSTED]->(t:Tweet)-[:LOCATED_AT]->(place:Place)\n",
    "        WHERE point.distance(t.location, center) < $radius_m\n",
    "        OPTIONAL MATCH (t)-[:TAGGED_WITH]->(h:Hashtag)\n",
    "\n",
    "        RETURN t.text                          AS text,\n",
    "               u.username                      AS author,\n",
    "               place.name                      AS place,\n",
    "               collect(DISTINCT h.name)        AS hashtags,\n",
    "               t.likes                         AS likes,\n",
    "               round(point.distance(t.location, center) / 1000.0, 2) AS distance_km\n",
    "        ORDER BY distance_km ASC\n",
    "        LIMIT $limit\n",
    "    \"\"\"\n",
    "    with driver.session(database=\"neo4j\") as session:\n",
    "        result = session.run(cypher, city=city_name, radius_m=radius_km * 1000, limit=limit)\n",
    "        return [dict(record) for record in result]\n",
    "\n",
    "\n",
    "# â”€â”€ Demo: find tweets near multiple cities â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "geo_demos = [\n",
    "    (\"San Francisco, CA\", 50),\n",
    "    (\"London, GB\", 30),\n",
    "]\n",
    "\n",
    "for city, radius in geo_demos:\n",
    "    results = find_tweets_near(city, radius_km=radius)\n",
    "    print(f\"ğŸ“ Tweets near {city} (within {radius} km):\\n\")\n",
    "    for i, r in enumerate(results, 1):\n",
    "        print(f\"  {i}. @{r['author']} â€” {r['distance_km']} km away\")\n",
    "        print(f\"     \\\"{r['text'][:80]}...\\\"\")\n",
    "        print(f\"     ğŸ·ï¸ {r['hashtags']}  â¤ï¸ {r['likes']} likes\\n\")\n",
    "    print()\n",
    "    print(\"-\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea56013c",
   "metadata": {
    "id": "ea56013c"
   },
   "source": [
    "### ğŸ—ºï¸ Which cities are nearest to each other?\n",
    "\n",
    "We can also compute **inter-city distances** directly in the graph to see how our Place nodes relate geographically."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "907c41a7",
   "metadata": {
    "id": "907c41a7"
   },
   "outputs": [],
   "source": [
    "# â”€â”€ Compute pairwise distances between all cities â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "with driver.session(database=\"neo4j\") as session:\n",
    "    result = session.run(\"\"\"\n",
    "        MATCH (a:Place), (b:Place)\n",
    "        WHERE a.name < b.name  // avoid duplicates\n",
    "        RETURN a.name AS city_a,\n",
    "               b.name AS city_b,\n",
    "               round(point.distance(a.location, b.location) / 1000.0, 0) AS distance_km\n",
    "        ORDER BY distance_km ASC\n",
    "    \"\"\")\n",
    "    print(\"â”€â”€ City-to-City Distances â”€â”€\\n\")\n",
    "    for record in result:\n",
    "        print(f\"  {record['city_a']:>20s}  â†”  {record['city_b']:<20s}  {record['distance_km']:>8.0f} km\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16e027c0",
   "metadata": {
    "id": "16e027c0"
   },
   "source": [
    "### ğŸ§  Geo-Augmented GraphRAG\n",
    "\n",
    "Combine **geospatial filtering + vector search + graph traversal** into a single retrieval function. This answers location-specific questions like:\n",
    "> *\"What are people near London saying about AI?\"*\n",
    "\n",
    "**How it works:**\n",
    "1. **Geo-filter**: Find tweets within a radius of a city using `point.distance()`\n",
    "2. **Vector search**: Among geo-filtered tweets, find semantically similar ones\n",
    "3. **Graph traversal**: Enrich with author, hashtags, and related content\n",
    "4. **LLM generation**: Synthesize findings into a natural language answer\n",
    "\n",
    "**Tutorial Note:** We demonstrate 2 geo-scoped questions across different cities to show regional differences in discussions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afbabe48",
   "metadata": {
    "id": "afbabe48"
   },
   "outputs": [],
   "source": [
    "def geo_graph_rag_search(question: str, city_name: str, radius_km: int = 100, k: int = 5) -> list[dict]:\n",
    "    \"\"\"\n",
    "    Geo-filtered GraphRAG: vector search restricted to tweets\n",
    "    within `radius_km` of a city, then enriched via graph traversal.\n",
    "    \"\"\"\n",
    "    q_embedding = get_embeddings([question])[0]\n",
    "\n",
    "    cypher = \"\"\"\n",
    "        // Get city center\n",
    "        MATCH (p:Place {name: $city})\n",
    "        WITH p.location AS center\n",
    "\n",
    "        // Vector search â€” find semantically similar tweets\n",
    "        CALL db.index.vector.queryNodes($index, $k_broad, $embedding)\n",
    "        YIELD node AS tweet, score\n",
    "\n",
    "        // Geo filter â€” keep only tweets within radius\n",
    "        WHERE point.distance(tweet.location, center) < $radius_m\n",
    "\n",
    "        // Graph traversal â€” enrich\n",
    "        MATCH (author:User)-[:POSTED]->(tweet)\n",
    "        OPTIONAL MATCH (tweet)-[:LOCATED_AT]->(place:Place)\n",
    "        OPTIONAL MATCH (tweet)-[:TAGGED_WITH]->(hashtag:Hashtag)\n",
    "\n",
    "        RETURN tweet.text                       AS text,\n",
    "               score,\n",
    "               author.username                  AS author,\n",
    "               author.followers                 AS author_followers,\n",
    "               place.name                       AS location,\n",
    "               place.country                    AS country,\n",
    "               collect(DISTINCT hashtag.name)   AS hashtags,\n",
    "               tweet.likes                      AS likes,\n",
    "               tweet.retweets                   AS retweets,\n",
    "               round(point.distance(tweet.location, center) / 1000.0, 2) AS distance_km\n",
    "        ORDER BY score DESC\n",
    "        LIMIT $k\n",
    "    \"\"\"\n",
    "    with driver.session(database=\"neo4j\") as session:\n",
    "        # Search a broader pool then geo-filter down\n",
    "        result = session.run(\n",
    "            cypher,\n",
    "            city=city_name,\n",
    "            radius_m=radius_km * 1000,\n",
    "            index=INDEX_NAME,\n",
    "            k_broad=k * 10,  # cast a wider net for vector search\n",
    "            k=k,\n",
    "            embedding=q_embedding,\n",
    "        )\n",
    "        return [dict(record) for record in result]\n",
    "\n",
    "\n",
    "def geo_graph_rag_answer(question: str, city_name: str, radius_km: int = 100, k: int = 5) -> str:\n",
    "    \"\"\"Geo-filtered GraphRAG + Gemini LLM.\"\"\"\n",
    "    results = geo_graph_rag_search(question, city_name, radius_km, k)\n",
    "    if not results:\n",
    "        return f\"No tweets found near {city_name} matching your question.\"\n",
    "\n",
    "    context_lines = []\n",
    "    for i, r in enumerate(results, 1):\n",
    "        context_lines.append(\n",
    "            f\"Tweet {i} (similarity={r['score']:.3f}, {r['distance_km']} km from {city_name}):\\n\"\n",
    "            f\"  Text: \\\"{r['text']}\\\"\\n\"\n",
    "            f\"  Author: @{r['author']} ({r['author_followers']} followers)\\n\"\n",
    "            f\"  Location: {r['location']}, {r['country']}\\n\"\n",
    "            f\"  Hashtags: {', '.join(r['hashtags'])}\\n\"\n",
    "            f\"  Engagement: {r['likes']} likes, {r['retweets']} retweets\"\n",
    "        )\n",
    "    context = \"\\n\\n\".join(context_lines)\n",
    "\n",
    "    response = client.models.generate_content(\n",
    "        model=LLM_MODEL,\n",
    "        contents=f\"Context:\\n{context}\\n\\nQuestion: {question}\",\n",
    "        config=genai.types.GenerateContentConfig(\n",
    "            system_instruction=SYSTEM_PROMPT,\n",
    "            temperature=0.2,\n",
    "        ),\n",
    "    )\n",
    "    return response.text\n",
    "\n",
    "\n",
    "print(\"âœ… Geo-augmented GraphRAG ready.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efa6b211",
   "metadata": {
    "id": "efa6b211"
   },
   "outputs": [],
   "source": [
    "# â”€â”€ Geo-scoped questions across multiple cities â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "geo_rag_demos = [\n",
    "    (\"What are people saying about AI and machine learning?\", \"London, UK\", 100),\n",
    "    (\"What do people think about cloud computing and serverless?\", \"San Francisco, CA\", 80),\n",
    "]\n",
    "\n",
    "for question, city, radius in geo_rag_demos:\n",
    "    print(\"=\" * 70)\n",
    "    print(f\"â“ {question}\")\n",
    "    print(f\"ğŸ“ Within {radius} km of {city}\")\n",
    "    print(\"=\" * 70)\n",
    "    \n",
    "    try:\n",
    "        print(geo_graph_rag_answer(question, city, radius_km=radius))\n",
    "    except Exception as e:\n",
    "        print(f\"âš ï¸ Error: {e}\")\n",
    "    \n",
    "    # Delay to avoid rate limits\n",
    "    if geo_rag_demos.index((question, city, radius)) < len(geo_rag_demos) - 1:\n",
    "        print(\"\\nâ³ Waiting 20 seconds to avoid rate limits...\")\n",
    "        time.sleep(20)\n",
    "    \n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "416982ac",
   "metadata": {
    "id": "416982ac"
   },
   "source": [
    "---\n",
    "## ğŸ§¹ Cleanup\n",
    "\n",
    "Close the Neo4j driver when finished."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be8aed44",
   "metadata": {
    "id": "be8aed44"
   },
   "outputs": [],
   "source": [
    "driver.close()\n",
    "print(\"âœ… Neo4j connection closed.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25c5e48a",
   "metadata": {
    "id": "25c5e48a"
   },
   "source": [
    "---\n",
    "## ğŸ Conclusion\n",
    "\n",
    "We built a complete **GraphRAG** pipeline from the social media knowledge graph:\n",
    "\n",
    "| Component | What it does |\n",
    "|-----------|-------------|\n",
    "| **Vector Embeddings** | Encode tweet text into 3072-d vectors via Gemini |\n",
    "| **Neo4j Vector Index** | Fast cosine-similarity search over tweet embeddings |\n",
    "| **Graph Traversal** | Enrich results with author, location, hashtags, and related tweets |\n",
    "| **LLM Generation** | Produce grounded answers from the enriched context |\n",
    "| **Cypher Generation** | Let the LLM write graph queries for analytical questions |\n",
    "| **Geospatial Queries** | Filter tweets by proximity using `point.distance()` |\n",
    "| **Geo-Augmented GraphRAG** | Combine vector search + geo-filter + graph traversal |\n",
    "\n",
    "**Key Insight:** GraphRAG produces *richer, more accurate* answers than plain vector search because it leverages the **structural relationships** between entities â€” not just text similarity. Adding geospatial filtering lets you scope answers to a specific region.\n",
    "\n",
    "**ğŸš€ API Usage (Tutorial-Optimized):**\n",
    "- Test queries enabled in Steps 5 & 6 (single queries to demonstrate functionality)\n",
    "- Step 8: 2 comparison questions (traditional RAG vs GraphRAG)\n",
    "- Step 10: 2 analytical questions (Cypher-augmented generation)\n",
    "- Step 11: 2 geospatial demos + 2 geo-augmented GraphRAG queries\n",
    "- Rate-limiting delays (20s between multi-call operations) to avoid quota issues\n",
    "- Balanced between comprehensive demonstration and API cost efficiency\n",
    "\n",
    "**Next Steps:**\n",
    "- Add **community detection** to find clusters of related users\n",
    "- Implement **hybrid search** (keyword + vector + graph)\n",
    "\n",
    "- Build a **Streamlit app** for interactive graph exploration- Add **temporal analysis** â€” how do topics evolve over time?"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "include_colab_link": true,
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
