{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "mount_file_id": "1HA2vITKPNEmtXlN6xknCeB_YUFcTiFHf",
      "authorship_tag": "ABX9TyPDy0ICrtL6yAIuNdbGLbD5",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/lbsocial/data-analysis-with-generative-ai/blob/main/multimodal_search_engine_tutorial.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# üê¶ Tutorial: Build a Multimodal \"Smart Search\" with MongoDB & AI\n",
        "\n",
        "In this tutorial, we will build a search engine that goes beyond simple keyword matching. Using **Vector Search** and **Multimodal AI**, we will create a system that allows users to search for tweets using **Text** or **Images**.\n",
        "\n",
        "**What you will learn:**\n",
        "1.  **Mock Data Generation:** How to create realistic social media data with Python.\n",
        "2.  **Multimodal Embeddings:** How to use OpenAI's **CLIP** model to understand both text and images.\n",
        "3.  **Vector Search:** How to store and search embeddings using **MongoDB Atlas**.\n",
        "4.  **Cross-Modal Retrieval:** How to search for images using text (and vice versa).\n",
        "\n",
        "**Prerequisites:**\n",
        "* A MongoDB Atlas database.\n",
        "* The connection string saved in Colab Secrets as `mongodb_connection`.\n",
        "\n",
        "## ‚ö†Ô∏è Important: Enable GPU Runtime\n",
        "To ensure the AI model loads and runs quickly, please enable the T4 GPU.\n",
        "\n",
        "1.  Click **Runtime** in the top menu.\n",
        "2.  Select **Change runtime type**.\n",
        "3.  Under **Hardware accelerator**, choose **T4 GPU**.\n",
        "4.  Click **Save**."
      ],
      "metadata": {
        "id": "eAVDWRAiTwmR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ‚öôÔ∏è Step 1: Environment Setup\n",
        "We need to install the `sentence-transformers` library to load our AI model, and `pymongo` to interact with the database. We will also load the **CLIP** model, which is designed to map text and images into the same \"vector space,\" enabling us to compare them mathematically.\n",
        "\n"
      ],
      "metadata": {
        "id": "xrZkvk0BRhMA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install sentence-transformers pymongo pillow requests -q"
      ],
      "metadata": {
        "id": "_jAtCTI5R42N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We use **OpenAI's CLIP model** via the `sentence-transformers` library.\n",
        "\n",
        "**Why CLIP?**\n",
        "It aligns text and images in the same \"vector space,\" meaning the math for the word \"dog\" is similar to the math for a *picture* of a dog. This enables **Cross-Modal Search** (searching for images using text).\n",
        "\n",
        "**Technical Specs:**\n",
        "* **Model:** `clip-ViT-B-32` (A vision transformer trained on 32x32 pixel patches).\n",
        "* **Library:** `sentence-transformers` (Handles the complex image processing automatically).\n",
        "\n"
      ],
      "metadata": {
        "id": "sOJigPG8T0-o"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sentence_transformers import SentenceTransformer\n",
        "import time\n",
        "from PIL import Image\n",
        "\n",
        "print(\"‚è≥ Loading CLIP model... (this may take a moment)\")\n",
        "# We use CLIP because it understands both Text and Images in the same vector space\n",
        "model = SentenceTransformer('clip-ViT-B-32')\n",
        "Image.MAX_IMAGE_PIXELS = None\n",
        "print(\"‚úÖ Model loaded!\")"
      ],
      "metadata": {
        "id": "SflGYLoYT3r1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### üñºÔ∏è Visualizing the Vector Space\n",
        "\n",
        "The illustration below demonstrates the core magic of **Multimodal Embeddings**:\n",
        "\n",
        "1.  **Dual Inputs:** We feed two completely different types of data‚Äîan **Image** (pixels of a cat) and **Text** (\"A fluffy cat\")‚Äîinto the same AI model.\n",
        "2.  **Translation:** The model converts both inputs into **Vectors** (lists of numbers).\n",
        "3.  **The Shared Space:** Notice that the **Blue Dot** (Image) and the **Green Dot** (Text) land very close to each other because they represent the same concept.\n",
        "\n",
        "<img src=\"https://raw.githubusercontent.com/lbsocial/data-analysis-with-generative-ai/main/image/Gemini_Generated_Image_rj1wcvrj1wcvrj1w.png\" width=\"600\" alt=\"Shared Vector Space\">\n",
        "\n",
        "*(Source: LBSocial)*"
      ],
      "metadata": {
        "id": "go_UijElxk9L"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Connect to MongoDB & Twitter\n",
        "Make sure you have your secrets `mongodb_connection` set up in the Colab side panel."
      ],
      "metadata": {
        "id": "a-FM5TM3UC43"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import userdata\n",
        "from pymongo import MongoClient\n",
        "\n",
        "# Setup MongoDB Connection\n",
        "mongo_uri = userdata.get('mongodb_connection')\n",
        "mongo_client = MongoClient(mongo_uri)\n",
        "\n",
        "# Connect to the specific collection\n",
        "db = mongo_client['demo']\n",
        "collection = db['tweet_collection']\n",
        "\n",
        "print(\"‚úÖ Connected to MongoDB collection: demo.tweet_collection\")"
      ],
      "metadata": {
        "id": "-RKakS2TUEk_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## üõ†Ô∏è Step 2: Generate Synthetic Tweets\n",
        "\n",
        "\n",
        "In a real-world application, you would connect this system to the Twitter API to index your own top tweets or timeline.\n",
        "\n",
        "**Option A: Use Your Own Data**\n",
        "If you have a collection of tweets (with image URLs), you can use them here!\n",
        "\n",
        "**Option B: Generate Mock Data**\n",
        "If you **don't have tweets**, simply run the code below. We will create tweets across distinct categories (Tech, Animals, Food) to test if our search engine can accurately distinguish between visual and semantic concepts. Each tweet will follow the standard Twitter data structure (JSON with `id`, `text`, and `entities`)."
      ],
      "metadata": {
        "id": "kpv5JhfqVFSe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# --- 1. SETUP & IMPORTS ---\n",
        "import datetime\n",
        "import random\n",
        "from google.colab import userdata\n",
        "from pymongo import MongoClient\n",
        "\n",
        "# Connect to MongoDB\n",
        "try:\n",
        "    mongo_uri = userdata.get('mongodb_connection')\n",
        "    client = MongoClient(mongo_uri)\n",
        "    db = client['demo']\n",
        "    collection = db['tweet_collection']\n",
        "    print(\"‚úÖ Connected to MongoDB collection: demo.tweet_collection\")\n",
        "except Exception as e:\n",
        "    print(f\"‚ùå Connection Error: {e}\")\n",
        "\n",
        "# --- 2. CONFIG: DIVERSE DATA BANKS ---\n",
        "\n",
        "# A. Image URLs (Unsplash) - We cycle through these\n",
        "image_bank = {\n",
        "    \"tech_setup\": [\n",
        "        \"https://images.unsplash.com/photo-1595225476474-87563907a212\", # Keyboard\n",
        "        \"https://images.unsplash.com/photo-1593640408182-31c70c8268f5\", # PC Setup\n",
        "        \"https://images.unsplash.com/photo-1587202372775-e229f172b9d7\", # Monitor\n",
        "        \"https://images.unsplash.com/photo-1550745165-9bc0b252726f\",  # Retro\n",
        "        \"https://images.unsplash.com/photo-1527443224154-c4a3942d3acf\"  # Mouse\n",
        "    ],\n",
        "    \"animals\": [\n",
        "        \"https://images.unsplash.com/photo-1552053831-71594a27632d\", # Retriever\n",
        "        \"https://images.unsplash.com/photo-1514888286974-6c03e2ca1dba\", # Cat\n",
        "        \"https://images.unsplash.com/photo-1583511655857-d19b40a7a54e\", # Dog face\n",
        "        \"https://images.unsplash.com/photo-1573865526739-10659fec78a5\", # Sleeping cat\n",
        "        \"https://images.unsplash.com/photo-1537151608828-ea2b11777ee8\"  # Puppy\n",
        "    ],\n",
        "    \"food\": [\n",
        "        \"https://images.unsplash.com/photo-1511920170033-f8396924c348\", # Latte\n",
        "        \"https://images.unsplash.com/photo-1579871494447-9811cf80d66c\", # Sushi\n",
        "        \"https://images.unsplash.com/photo-1565299624946-b28f40a0ae38\", # Pizza\n",
        "        \"https://images.unsplash.com/photo-1482049016688-2d3e1b311543\", # Toast\n",
        "        \"https://images.unsplash.com/photo-1484723091739-30a097e8f929\"  # Burger\n",
        "    ]\n",
        "}\n",
        "\n",
        "# B. Text Content (Human-written variety for better semantic search)\n",
        "text_bank = {\n",
        "    \"tech_setup\": [\n",
        "        \"Just installed my new RTX 4090. The frame rates are buttery smooth!\",\n",
        "        \"Why is cable management so hard? I spent 3 hours just hiding wires.\",\n",
        "        \"My mechanical keyboard is way too loud for late night gaming sessions.\",\n",
        "        \"Finally upgraded to a dual monitor setup. Productivity increased by 200%.\",\n",
        "        \"Building a custom water-cooled PC is terrifying but worth it.\",\n",
        "        \"Does anyone else hate Windows 11 updates? They break everything.\",\n",
        "        \"Loving the RGB aesthetic on this new mousepad.\",\n",
        "        \"My laptop is overheating again. Time to clean the fans.\",\n",
        "        \"Testing out the new VR headset. Virtual reality is finally getting good.\",\n",
        "        \"Is it worth buying a curved monitor for coding?\",\n",
        "        \"My wifi speed is terrible today, I can't stream anything.\",\n",
        "        \"Just bought a standing desk. My back feels so much better.\",\n",
        "        \"Reviewing the latest tech gadgets on my blog tonight.\",\n",
        "        \"The battery life on this new device is actually impressive.\",\n",
        "        \"Nothing beats a clean, minimalist desk setup.\"\n",
        "    ],\n",
        "    \"animals\": [\n",
        "        \"My golden retriever is afraid of the vacuum cleaner. Poor guy!\",\n",
        "        \"Woke up to my cat sleeping on my face. Best alarm clock ever.\",\n",
        "        \"Took the dog to the beach today. He tried to eat the ocean.\",\n",
        "        \"Adopting a rescue kitten was the best decision I made this year.\",\n",
        "        \"Why do dogs chase their own tails? It's hilarious.\",\n",
        "        \"My parrot learned to mimic the microwave beep. It's so confusing.\",\n",
        "        \"Walking the dog in the rain is not my favorite activity.\",\n",
        "        \"Look at those puppy eyes! I can't say no to him.\",\n",
        "        \"My cat knocked a glass of water onto my laptop. Chaos ensues.\",\n",
        "        \"Spending the weekend hiking with my furry best friend.\",\n",
        "        \"Does your pet have a favorite toy they carry everywhere?\",\n",
        "        \"Watching birds in the garden is surprisingly relaxing.\",\n",
        "        \"My hamster ran on his wheel for 4 hours straight last night.\",\n",
        "        \"Just got back from the vet. Clean bill of health for the pup!\",\n",
        "        \"Cuddling with my cat after a long day is pure therapy.\"\n",
        "    ],\n",
        "    \"food\": [\n",
        "        \"This spicy ramen is clearing my sinuses instantly! So good.\",\n",
        "        \"Nothing beats the smell of fresh coffee and croissants in the morning.\",\n",
        "        \"Tried making sushi at home. It looks ugly but tastes amazing.\",\n",
        "        \"Best burger in town is definitely at that new downtown spot.\",\n",
        "        \"I could eat avocado toast for every meal of the day.\",\n",
        "        \"Craving some authentic Italian pasta right now.\",\n",
        "        \"This chocolate cake is way too rich, but I'm eating it anyway.\",\n",
        "        \"Trying to eat healthy, so I made a giant salad. It needs more dressing.\",\n",
        "        \"Ordering late-night pizza because I'm too lazy to cook.\",\n",
        "        \"Freshly squeezed orange juice is a game changer.\",\n",
        "        \"The seafood platter at this restaurant is massive!\",\n",
        "        \"Baking cookies for the holiday party. hope they don't burn.\",\n",
        "        \"I need a strong espresso to survive this Monday afternoon.\",\n",
        "        \"Enjoying a glass of red wine with a cheese board.\",\n",
        "        \"Why does pineapple on pizza cause so many arguments?\"\n",
        "    ],\n",
        "    \"coding_text\": [\n",
        "        \"Spent 4 hours debugging a missing semicolon. I love programming.\",\n",
        "        \"Git merge conflict: 1, Me: 0. I hate this.\",\n",
        "        \"Deploying to production on a Friday. Living dangerously!\",\n",
        "        \"Python is so much more readable than C++. Change my mind.\",\n",
        "        \"Finally fixed that recursion error! I feel like a wizard.\",\n",
        "        \"My SQL query is taking forever to run. Need to index these tables.\",\n",
        "        \"Learning Rust is humbling. The compiler is so strict.\",\n",
        "        \"Stack Overflow is down. Guess I can't do my job today.\",\n",
        "        \"Refactoring legacy code is a nightmare. Who wrote this mess?\",\n",
        "        \"Docker containers are failing to spin up. Send help.\",\n",
        "        \"Just pushed my first open source contribution! So proud.\",\n",
        "        \"Writing documentation is boring, but future me will be thankful.\",\n",
        "        \"Why does this code work on localhost but fail on the server?\",\n",
        "        \"Automating my boring tasks with a simple shell script.\",\n",
        "        \"Unit tests are all passing. I am suspicious...\"\n",
        "    ]\n",
        "}\n",
        "\n",
        "# --- 3. GENERATION LOOP (With Shuffle) ---\n",
        "print(\"üöÄ Generating 60 Unique Mock Tweets...\")\n",
        "\n",
        "docs_to_insert = []\n",
        "categories = [\"tech_setup\", \"animals\", \"food\", \"coding_text\"]\n",
        "\n",
        "for category in categories:\n",
        "    print(f\"   Processing category: {category}...\")\n",
        "\n",
        "    # Get the text list and shuffle it so it's random\n",
        "    texts = text_bank[category]\n",
        "    random.shuffle(texts)\n",
        "\n",
        "    # Get images (if applicable)\n",
        "    images = image_bank.get(category, [])\n",
        "\n",
        "    # Generate 15 tweets per category (matching the text bank size)\n",
        "    for i in range(15):\n",
        "        text = texts[i]\n",
        "\n",
        "        # Assign Image URL (Cycle through available images)\n",
        "        if category == \"coding_text\":\n",
        "            img_url = None\n",
        "            entities = {}\n",
        "        else:\n",
        "            img_url = images[i % len(images)]\n",
        "            # Standard Twitter Media Structure\n",
        "            entities = {\n",
        "                \"media\": [{\n",
        "                    \"media_url_https\": img_url,\n",
        "                    \"type\": \"photo\",\n",
        "                    \"display_url\": \"pic.twitter.com/xyz\"\n",
        "                }]\n",
        "            }\n",
        "\n",
        "        # Generate Fake ID & Timestamp\n",
        "        fake_id = str(random.randint(1000000000000000000, 1999999999999999999))\n",
        "        created_at = datetime.datetime.now().isoformat()\n",
        "\n",
        "        # Final Object\n",
        "        tweet_doc = {\n",
        "            \"id\": fake_id,\n",
        "            \"text\": text,\n",
        "            \"created_at\": created_at,\n",
        "            \"entities\": entities,\n",
        "            \"category\": category # Helper field for tutorial\n",
        "        }\n",
        "\n",
        "        docs_to_insert.append(tweet_doc)\n",
        "\n",
        "# --- 4. INSERT ---\n",
        "if docs_to_insert:\n",
        "    collection.delete_many({})\n",
        "    collection.insert_many(docs_to_insert)\n",
        "    print(\"-\" * 40)\n",
        "    print(f\"üéâ SUCCESS! Stored {len(docs_to_insert)} high-quality tweets.\")\n",
        "    print(\"   Example Text: \" + docs_to_insert[0]['text'])\n",
        "    print(\"-\" * 40)"
      ],
      "metadata": {
        "id": "jkDBr3ikVHcH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Verify the Dataset**\n",
        "\n",
        "Before generating embeddings, let's peek into our MongoDB collection to ensure the data looks correct.\n",
        "\n",
        "We will run a simple **Aggregation Query** to:\n",
        "1.  **Group** the tweets by their category (e.g., Tech, Animals, Food).\n",
        "2.  **Count** how many tweets are in each category (should be 15 each).\n",
        "3.  **Preview** a few examples of the text and image URLs to make sure they were generated properly."
      ],
      "metadata": {
        "id": "VhimR4aF6U7l"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# --- 1. SETUP ---\n",
        "from google.colab import userdata\n",
        "from pymongo import MongoClient\n",
        "\n",
        "# Connect\n",
        "try:\n",
        "    mongo_uri = userdata.get('mongodb_connection')\n",
        "    client = MongoClient(mongo_uri)\n",
        "    db = client['demo']\n",
        "    collection = db['tweet_collection']\n",
        "    print(\"‚úÖ Connected to MongoDB\")\n",
        "except Exception as e:\n",
        "    print(f\"‚ùå Connection Error: {e}\")\n",
        "\n",
        "# --- 2. SUMMARY QUERY (AGGREGATION) ---\n",
        "print(\"\\nüìä DATASET SUMMARY:\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "pipeline = [\n",
        "    {\n",
        "        \"$group\": {\n",
        "            \"_id\": \"$category\",\n",
        "            \"count\": { \"$sum\": 1 },\n",
        "            # Collect the first 3 text examples\n",
        "            \"sample_texts\": { \"$push\": \"$text\" },\n",
        "            # Collect the first 3 image examples (extracting from the nested entities)\n",
        "            \"sample_images\": { \"$push\": { \"$arrayElemAt\": [\"$entities.media.media_url_https\", 0] } }\n",
        "        }\n",
        "    }\n",
        "]\n",
        "\n",
        "results = list(collection.aggregate(pipeline))\n",
        "\n",
        "# --- 3. DISPLAY RESULTS ---\n",
        "for cat_data in results:\n",
        "    category = cat_data['_id']\n",
        "    count = cat_data['count']\n",
        "    texts = cat_data['sample_texts'][:3] # Show only top 3\n",
        "\n",
        "    # Filter out None/Null images (e.g., for coding_text)\n",
        "    images = [img for img in cat_data['sample_images'] if img][:2] # Show only top 2\n",
        "\n",
        "    print(f\"üìÇ CATEGORY: {category} ({count} docs)\")\n",
        "\n",
        "    print(\"   üìù Text Examples:\")\n",
        "    for t in texts:\n",
        "        print(f\"      - \\\"{t[:60]}...\\\"\") # Truncate for cleaner view\n",
        "\n",
        "    if images:\n",
        "        print(\"   üñºÔ∏è  Image Examples:\")\n",
        "        for img in images:\n",
        "            print(f\"      - {img}\")\n",
        "    else:\n",
        "        print(\"   üñºÔ∏è  No Images (Expected for this category)\")\n",
        "\n",
        "    print(\"-\" * 60)"
      ],
      "metadata": {
        "id": "-Y6K3ZYDeHyS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## üß† Step 3: The \"Split Strategy\" for Embeddings\n",
        "\n",
        "This is the core logic of our Multimodal engine. We treat a tweet as two objects:\n",
        "\n",
        "1.  **Text Vector:** Represents the meaning of the text.\n",
        "2.  **Image Vector:** Represents the visual content of the image.\n",
        "\n",
        "By storing these separately, we can match a user's query against *either* the text *or* the image.\n",
        "\n",
        "<img src=\"https://github.com/lbsocial/data-analysis-with-generative-ai/blob/main/image/Gemini_Generated_Image_bctu0ubctu0ubctu.png?raw=true\" width=\"600\" alt=\"Split Strategy\">\n",
        "\n",
        "*(Source: LBSocial)*"
      ],
      "metadata": {
        "id": "hbei22Y5YoHg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "from PIL import Image\n",
        "from io import BytesIO\n",
        "from sentence_transformers import SentenceTransformer\n",
        "from google.colab import userdata\n",
        "from pymongo import MongoClient\n",
        "\n",
        "\n",
        "# --- 1. FETCH RAW DATA ---\n",
        "raw_tweets = list(collection.find({}))\n",
        "print(f\"üìÇ Found {len(raw_tweets)} raw tweets to process.\")\n",
        "\n",
        "# --- 2. EMBEDDING LOOP ---\n",
        "vector_documents = []\n",
        "print(\"üöÄ Starting Embedding Process...\")\n",
        "\n",
        "for i, tweet in enumerate(raw_tweets):\n",
        "    if i % 10 == 0 and i > 0: print(f\"   ... processed {i} tweets\")\n",
        "\n",
        "    # Extract Fields (Safely)\n",
        "    tweet_id = tweet.get('id')\n",
        "    text_content = tweet.get('text')\n",
        "\n",
        "    # Dig for Image URL in the 'entities' structure\n",
        "    image_url = None\n",
        "    entities = tweet.get('entities', {})\n",
        "    if 'media' in entities and len(entities['media']) > 0:\n",
        "        image_url = entities['media'][0]['media_url_https']\n",
        "\n",
        "    # A. TEXT EMBEDDING (Always exists)\n",
        "    try:\n",
        "        text_emb = model.encode(text_content).tolist()\n",
        "\n",
        "        vector_documents.append({\n",
        "            \"original_id\": tweet_id,\n",
        "            \"category\": tweet.get('category'), # Keep category for tutorial\n",
        "            \"media_type\": \"text\",\n",
        "            \"text\": text_content,\n",
        "            \"image_url\": image_url,\n",
        "            \"embedding\": text_emb\n",
        "        })\n",
        "    except Exception as e:\n",
        "        print(f\"   ‚ö†Ô∏è Text Error on ID {tweet_id}: {e}\")\n",
        "\n",
        "    # B. IMAGE EMBEDDING (If exists)\n",
        "    if image_url:\n",
        "        try:\n",
        "            # Download\n",
        "            response = requests.get(image_url, timeout=5)\n",
        "            if response.status_code == 200:\n",
        "                img = Image.open(BytesIO(response.content))\n",
        "\n",
        "                # Generate Vector\n",
        "                img_emb = model.encode(img).tolist()\n",
        "\n",
        "                vector_documents.append({\n",
        "                    \"original_id\": tweet_id,\n",
        "                    \"category\": tweet.get('category'),\n",
        "                    \"media_type\": \"image\",\n",
        "                    \"text\": text_content,\n",
        "                    \"image_url\": image_url,\n",
        "                    \"embedding\": img_emb\n",
        "                })\n",
        "        except Exception as e:\n",
        "            # If an image is corrupt or too big for Colab RAM, skip it\n",
        "            print(f\"   ‚ö†Ô∏è Image skipped for ID {tweet_id}: {e}\")\n",
        "\n",
        "# --- 3. SAVE RESULTS ---\n",
        "if vector_documents:\n",
        "    collection.delete_many({})\n",
        "    collection.insert_many(vector_documents)\n",
        "    print(\"-\" * 40)\n",
        "    print(f\"üéâ DONE! Saved {len(vector_documents)} vector documents to MongoDB.\")\n",
        "    print(\"-\" * 40)\n",
        "else:\n",
        "    print(\"‚ùå No documents generated.\")"
      ],
      "metadata": {
        "id": "x23Cf70SbPnQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ‚ö° Step 4: Create Vector Search Index\n",
        "For MongoDB to perform fast vector searches, we must define an index.\n",
        "\n",
        "We configure the index to use **512 dimensions** (matching the output of the `clip-ViT-B-32` model) and **Cosine Similarity**, which is the standard metric for measuring distance between semantic vectors."
      ],
      "metadata": {
        "id": "fyxH5pdWcA20"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pymongo.operations import SearchIndexModel\n",
        "import time\n",
        "\n",
        "index_name = \"vector_index\"\n",
        "\n",
        "# 1. Define Index\n",
        "index_definition = {\n",
        "  \"fields\": [\n",
        "    {\n",
        "      \"type\": \"vector\",\n",
        "      \"path\": \"embedding\",\n",
        "      \"numDimensions\": 512,\n",
        "      \"similarity\": \"cosine\"\n",
        "    },\n",
        "    {\"type\": \"filter\", \"path\": \"category\"},\n",
        "    {\"type\": \"filter\", \"path\": \"media_type\"}\n",
        "  ]\n",
        "}\n",
        "\n",
        "# 2. Create Index\n",
        "print(\"‚è≥ Creating Vector Search Index...\")\n",
        "try:\n",
        "    collection.create_search_index(\n",
        "        model=SearchIndexModel(definition=index_definition, name=index_name, type=\"vectorSearch\")\n",
        "    )\n",
        "    print(\"‚úÖ Index creation command sent.\")\n",
        "except Exception as e:\n",
        "    print(f\"‚ö†Ô∏è Index might already exist: {e}\")\n",
        "\n",
        "# 3. Poll for Readiness\n",
        "print(\"‚è≥ Waiting for index to be queryable...\")\n",
        "while True:\n",
        "    indices = list(collection.list_search_indexes(index_name))\n",
        "    if indices and indices[0].get('queryable'):\n",
        "        print(\"üéâ Index is READY!\")\n",
        "        break\n",
        "    time.sleep(5)"
      ],
      "metadata": {
        "id": "Xuoxe9WwcC4l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## üîç Step 5: Define the \"Double-Tap\" Search Logic\n",
        "\n",
        "To fix the **\"Modality Gap\"** (where text and images group separately), we use a **Double-Tap Strategy**:\n",
        "\n",
        "1.  **Search A:** Force the database to find the best **Text** matches.\n",
        "2.  **Search B:** Force the database to find the best **Image** matches.\n",
        "3.  **Merge:** Combine both sets to guarantee a rich result.\n",
        "\n",
        "<img src=\"https://github.com/lbsocial/data-analysis-with-generative-ai/blob/main/image/Gemini_Generated_Image_mqvyn0mqvyn0mqvy.png?raw=true\" width=\"600\" alt=\"Double Tap Strategy\">\n",
        "\n",
        "*(Source: LBSocial)*"
      ],
      "metadata": {
        "id": "AH9OvGhWckOL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "from PIL import Image\n",
        "from io import BytesIO\n",
        "\n",
        "def mixed_search(query, num_results=1):\n",
        "    # 1. DETECT INPUT & ENCODE\n",
        "    if query.startswith(\"http\"):\n",
        "        print(f\"üñºÔ∏è  Query: [Image URL]\")\n",
        "        try:\n",
        "            response = requests.get(query, stream=True)\n",
        "            img = Image.open(response.raw)\n",
        "            query_vector = model.encode(img).tolist()\n",
        "        except:\n",
        "            print(\"‚ùå Error loading image\")\n",
        "            return\n",
        "    else:\n",
        "        print(f\"üìù Query: '{query}'\")\n",
        "        query_vector = model.encode(query).tolist()\n",
        "\n",
        "    # --- 2. RUN TWO SEPARATE SEARCHES ---\n",
        "\n",
        "    # SEARCH A: Find only TEXT matches\n",
        "    pipeline_text = [\n",
        "        {\n",
        "            \"$vectorSearch\": {\n",
        "                \"index\": \"vector_index\",\n",
        "                \"path\": \"embedding\",\n",
        "                \"queryVector\": query_vector,\n",
        "                \"limit\": num_results,\n",
        "                \"numCandidates\": 100,       # <--- FIXED: This was missing!\n",
        "                \"filter\": { \"media_type\": \"text\" }\n",
        "            }\n",
        "        },\n",
        "        { \"$project\": { \"_id\": 0, \"text\": 1, \"category\": 1, \"score\": { \"$meta\": \"vectorSearchScore\" } } }\n",
        "    ]\n",
        "\n",
        "    # SEARCH B: Find only IMAGE matches\n",
        "    pipeline_image = [\n",
        "        {\n",
        "            \"$vectorSearch\": {\n",
        "                \"index\": \"vector_index\",\n",
        "                \"path\": \"embedding\",\n",
        "                \"queryVector\": query_vector,\n",
        "                \"limit\": num_results,\n",
        "                \"numCandidates\": 100,       # <--- FIXED: This was missing!\n",
        "                \"filter\": { \"media_type\": \"image\" }\n",
        "            }\n",
        "        },\n",
        "        { \"$project\": { \"_id\": 0, \"image_url\": 1, \"category\": 1, \"score\": { \"$meta\": \"vectorSearchScore\" } } }\n",
        "    ]\n",
        "\n",
        "    # Execute both\n",
        "    text_results = list(collection.aggregate(pipeline_text))\n",
        "    image_results = list(collection.aggregate(pipeline_image))\n",
        "\n",
        "    # --- 3. DISPLAY RESULTS ---\n",
        "    print(f\"\\nüîé MIXED RESULTS (Guaranteed):\")\n",
        "    print(\"=\" * 60)\n",
        "\n",
        "    print(f\"üìÑ BEST TEXT MATCHES:\")\n",
        "    if text_results:\n",
        "        for r in text_results:\n",
        "            print(f\"   ‚Ä¢ {r['score']:.4f} | {r['category']} | \\\"{r['text'][:50]}...\\\"\")\n",
        "    else:\n",
        "        print(\"   (No text matches found)\")\n",
        "\n",
        "    print(\"-\" * 60)\n",
        "\n",
        "    print(f\"üì∑ BEST IMAGE MATCHES:\")\n",
        "    if image_results:\n",
        "        for r in image_results:\n",
        "            print(f\"   ‚Ä¢ {r['score']:.4f} | {r['category']} | [Image Found]\")\n",
        "            if r.get('image_url'): print(f\"     Target: {r['image_url']}\")\n",
        "    else:\n",
        "        print(\"   (No image matches found)\")\n",
        "\n",
        "    print(\"=\" * 60)"
      ],
      "metadata": {
        "id": "Wa5RDXA7clws"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## üöÄ Step 6: Test the Engine\n",
        "Now it's time to verify our system. We will run three types of queries:\n",
        "1.  **Text-to-Text:** Searching \"Pizza\" to find tweets discussing food.\n",
        "2.  **Image-to-Image:** Searching with a photo of a dog to find similar pets.\n"
      ],
      "metadata": {
        "id": "R7hArfG6vR3L"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# --- TEST 1: Text-to-Text & Text-to-Image ---\n",
        "# Search: \"Pizza\"\n",
        "# Expectation: Finds text discussing food AND photos of pizza (even if the file name isn't \"pizza\").\n",
        "print(\">>> TEST 1: Text Search ('Pizza')\")\n",
        "mixed_search(\"pizza\")"
      ],
      "metadata": {
        "id": "9lXip5CijjLp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- TEST 2: Image-to-Image & Image-to-Text ---\n",
        "# Search: [Photo of a Dog]\n",
        "# Expectation: Finds similar dog photos AND text tweets about \"puppies\" or \"retrievers\".\n",
        "print(\"\\n>>> TEST 2: Image Search (Using a URL of a Dog)\")\n",
        "dog_img_url = \"https://images.unsplash.com/photo-1558788353-f76d92427f16\"\n",
        "mixed_search(dog_img_url)"
      ],
      "metadata": {
        "id": "2x2DY7jVjout"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## üéì Conclusion & References\n",
        "\n",
        "Congratulations! You have successfully built a **Multimodal Search Engine**.\n",
        "\n",
        "You have moved beyond simple keyword matching to creating a system that understands **concepts**. It knows that a picture of a keyboard is related to the text \"fast computer,\" and it can bridge the gap between images and text using Vector Search.\n",
        "\n",
        "### üìö References & Resources\n",
        "* **Tutorial Source:** [LBSocial](https://lbsocial.net)\n",
        "* **The AI Model:** [OpenAI CLIP (Hugging Face)](https://huggingface.co/sentence-transformers/clip-ViT-B-32)\n",
        "* **The Library:** [Sentence-Transformers Documentation](https://www.sbert.net/examples/applications/image-search/README.html)\n",
        "* **The Database:** [MongoDB Atlas Vector Search](https://www.mongodb.com/products/platform/atlas-vector-search)\n",
        "* **Concept Paper:** [Learning Transferable Visual Models From Natural Language Supervision](https://arxiv.org/abs/2103.00020)"
      ],
      "metadata": {
        "id": "X0Ow3DBNwjTp"
      }
    }
  ]
}