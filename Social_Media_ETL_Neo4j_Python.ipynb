{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "mount_file_id": "1DRp7wHf4FQ5mLt-X1j3mLq45IkSFWg0p",
      "authorship_tag": "ABX9TyPtsh84mLzrToUU1dQgCX7r",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/lbsocial/data-analysis-with-generative-ai/blob/main/Social_Media_ETL_Neo4j_Python.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# üöÄ Building a Social Media Knowledge Graph with Python & Neo4j\n",
        "\n",
        "**Turn flat data into a connected network.**\n",
        "\n",
        "In this tutorial, we will build a complete ETL (Extract, Transform, Load) pipeline. We will simulate a rich dataset of social media posts (users, tweets, hashtags, locations) and ingest them into a Graph Database to reveal the hidden relationships between them.\n",
        "\n",
        "**What we will build:**\n",
        "1.  **Synthetic Data Engine:** Use `Faker` to generate realistic, nested JSON data (similar to Twitter/X API v2).\n",
        "2.  **Context-Aware Content:** Instead of random gibberish, we will generate semantically consistent text (about AI, Databases, etc.) to prepare for future Vector Search analysis.\n",
        "3.  **High-Performance Ingestion:** Use the Neo4j Python Driver and Cypher `UNWIND` to batch load data efficiently.\n",
        "\n",
        "---\n",
        "### üõ†Ô∏è Step 1: Install Dependencies\n",
        "We need `neo4j` to connect to the database and `faker` to generate our synthetic data."
      ],
      "metadata": {
        "id": "7fbkD2sVlByi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pip install neo4j faker -q"
      ],
      "metadata": {
        "id": "SGe9BJ1Ca62K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ü¶ú Step 2: Generate Context-Aware Social Data\n",
        "In this step, we create the \"dummy\" dataset using Python and Faker.\n",
        "\n",
        "**The Strategy:**\n",
        "To make this data useful for **AI & Vector Search** later, we aren't just generating random noise. We are generating **Semantic Clusters**.\n",
        "* We define 4 core topics (Neo4j, AI, Python, Cloud).\n",
        "* We map specific sentences to each topic.\n",
        "* This ensures that when we visualize the data later, we will see clear groups of related content.\n",
        "\n",
        "**Visualizing the Data Structure:**\n",
        "We nest the **Author** and **Place** objects *inside* the Tweet JSON. This \"document-style\" structure is easier to pass to the database in one go.\n",
        "\n",
        "![Tweet Object Structure](https://github.com/lbsocial/data-analysis-with-generative-ai/blob/1660b372f63accb8e55ea4b439924ba764639182/image/Gemini_Generated_Image_kcezvkcezvkcezvk.png?raw=true)"
      ],
      "metadata": {
        "id": "ZUg1ixg7lKAc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "import random\n",
        "from faker import Faker\n",
        "import datetime\n",
        "\n",
        "fake = Faker()\n",
        "Faker.seed(42)\n",
        "\n",
        "# 1. Setup Static Data\n",
        "# FIXED: Added \"full_name\" key to all entries\n",
        "locations_db = [\n",
        "    {\"full_name\": \"New York, NY\", \"country\": \"US\", \"lon\": -74.00, \"lat\": 40.71},\n",
        "    {\"full_name\": \"London, UK\", \"country\": \"GB\", \"lon\": -0.12, \"lat\": 51.50},\n",
        "    {\"full_name\": \"Tokyo, JP\", \"country\": \"JP\", \"lon\": 139.69, \"lat\": 35.68},\n",
        "    {\"full_name\": \"San Francisco, CA\", \"country\": \"US\", \"lon\": -122.41, \"lat\": 37.77},\n",
        "    {\"full_name\": \"Paris, FR\", \"country\": \"FR\", \"lon\": 2.35, \"lat\": 48.85}\n",
        "]\n",
        "\n",
        "# DOMAIN SPECIFIC CONTENT (Enables Semantic Search later)\n",
        "topic_content = {\n",
        "    \"Neo4j\": [\n",
        "        \"Graph databases are game changers for handling complex relationships.\",\n",
        "        \"Just learned how to use Cypher query language, it is so intuitive!\",\n",
        "        \"Relational DBs struggle with joins, but graphs handle them naturally.\",\n",
        "        \"Building a recommendation engine is much easier with nodes and edges.\"\n",
        "    ],\n",
        "    \"AI\": [\n",
        "        \"The new Large Language Models are hallucinating less and reasoning more.\",\n",
        "        \"Generative AI is transforming how we write code every day.\",\n",
        "        \"Thinking about the ethics of autonomous agents in production.\",\n",
        "        \"Just deployed a new transformer model to the cloud.\"\n",
        "    ],\n",
        "    \"Python\": [\n",
        "        \"I love how clean Python syntax is for data science projects.\",\n",
        "        \"Pandas and NumPy are essential tools for any data engineer.\",\n",
        "        \"Debugging async code in Python can be tricky but worth it.\",\n",
        "        \"Automating my daily workflows with a simple Python script.\"\n",
        "    ],\n",
        "    \"Cloud\": [\n",
        "        \"Serverless architecture really cuts down on maintenance overhead.\",\n",
        "        \"Scaling kubernetes clusters across multiple regions today.\",\n",
        "        \"Cloud costs are getting high, need to optimize our storage buckets.\",\n",
        "        \"Deploying microservices to the edge for lower latency.\"\n",
        "    ]\n",
        "}\n",
        "\n",
        "hashtags_list = list(topic_content.keys())\n",
        "usernames = [\"Alice_Data\", \"Bob_Graphs\", \"Charlie_AI\", \"Dave_Dev\", \"Eve_Sec\"]\n",
        "\n",
        "# 2. Pre-generate Users (With 19-digit IDs)\n",
        "user_map = {}\n",
        "for username in usernames:\n",
        "    user_map[username] = {\n",
        "        \"id\": str(fake.unique.random_number(digits=19)), # Real ID Format\n",
        "        \"username\": username,\n",
        "        \"name\": fake.name(),\n",
        "        \"public_metrics\": {\n",
        "            \"followers_count\": random.randint(100, 50000),\n",
        "            \"following_count\": random.randint(10, 2000),\n",
        "            \"tweet_count\": random.randint(50, 5000)\n",
        "        }\n",
        "    }\n",
        "\n",
        "tweets = []\n",
        "print(\"Generating 100 Context-Aware Tweets...\")\n",
        "\n",
        "for i in range(100):\n",
        "    author = user_map[random.choice(usernames)]\n",
        "    city = random.choice(locations_db)\n",
        "\n",
        "    # Pick a Primary Topic to drive the text content\n",
        "    primary_topic = random.choice(hashtags_list)\n",
        "\n",
        "    # Generate meaningful text based on the topic\n",
        "    base_text = random.choice(topic_content[primary_topic])\n",
        "\n",
        "    # Add some random tags (ensure the primary topic is included)\n",
        "    tags = [primary_topic]\n",
        "    if random.random() > 0.5:\n",
        "        other_tags = [t for t in hashtags_list if t != primary_topic]\n",
        "        tags += random.sample(other_tags, k=random.randint(1, 2))\n",
        "\n",
        "    hashtag_entities = [{\"tag\": t} for t in tags]\n",
        "\n",
        "    # Generate IDs and Jitter GPS\n",
        "    tweet_id = str(fake.unique.random_number(digits=19))\n",
        "    tweet_lon = city[\"lon\"] + random.uniform(-0.05, 0.05)\n",
        "    tweet_lat = city[\"lat\"] + random.uniform(-0.05, 0.05)\n",
        "\n",
        "    tweet = {\n",
        "        \"id\": tweet_id,\n",
        "        \"text\": base_text + \" \" + \" \".join([f\"#{t}\" for t in tags]),\n",
        "        \"created_at\": fake.date_time_between(start_date=\"-30d\", end_date=\"now\").isoformat(),\n",
        "        \"author_id\": author[\"id\"],\n",
        "        \"public_metrics\": {\n",
        "            \"like_count\": random.randint(0, 1000),\n",
        "            \"retweet_count\": random.randint(0, 500),\n",
        "            \"reply_count\": random.randint(0, 50)\n",
        "        },\n",
        "        \"entities\": {\n",
        "            \"hashtags\": hashtag_entities\n",
        "        },\n",
        "        \"geo\": {\n",
        "            \"coordinates\": {\"type\": \"Point\", \"coordinates\": [tweet_lon, tweet_lat]}\n",
        "        },\n",
        "        \"place\": {\n",
        "            \"full_name\": city[\"full_name\"],\n",
        "            \"country\": city[\"country\"],\n",
        "            \"centroid\": [city[\"lon\"], city[\"lat\"]]\n",
        "        },\n",
        "        \"__expansion_author\": author\n",
        "    }\n",
        "    tweets.append(tweet)\n",
        "\n",
        "with open(\"dummy_tweets.json\", \"w\") as f:\n",
        "    json.dump(tweets, f, indent=2)\n",
        "\n",
        "print(\"‚úÖ Generated context-aware tweets (Ready for Vector Search).\")"
      ],
      "metadata": {
        "id": "0UznNp3qhjI1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with open(\"dummy_tweets.json\", \"r\") as f:\n",
        "    data = json.load(f)\n",
        "    for tweet in data:\n",
        "        print(json.dumps(tweet, indent=2))\n",
        "        break"
      ],
      "metadata": {
        "id": "-ghF1CJFh0pn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### üîå Step 3: Connect to Neo4j\n",
        "We use the official Python driver.\n",
        "* **Note:** Ensure you have added your `URI` and `password` to the Colab \"Secrets\" tab."
      ],
      "metadata": {
        "id": "2dIRpCXzl5FM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import userdata\n",
        "\n",
        "password = userdata.get('password')\n",
        "# --- CONFIGURATION ---\n",
        "URI = userdata.get('URI')\n",
        "AUTH = (\"neo4j\", f\"{password}\")\n"
      ],
      "metadata": {
        "id": "VJ5gz6k5aZh3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "try:\n",
        "    with GraphDatabase.driver(URI, auth=AUTH) as driver:\n",
        "        driver.verify_connectivity()\n",
        "        print(\"‚úÖ SUCCESS! Connected to Neo4j.\")\n",
        "except Exception as e:\n",
        "    print(f\"‚ùå ERROR: {e}\")"
      ],
      "metadata": {
        "id": "RWqeD1xycvXb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### üì• Step 4: Ingest Data (Graph Construction)\n",
        "We use **Cypher** (Neo4j's query language) to map our JSON data into nodes and relationships.\n",
        "\n",
        "**The Blueprint:**\n",
        "The image below shows the exact graph structure our query will build. Notice how we split the single Tweet object into four distinct, connected nodes.\n",
        "\n",
        "![Graph Data Model](https://github.com/lbsocial/data-analysis-with-generative-ai/blob/1660b372f63accb8e55ea4b439924ba764639182/image/Gemini_Generated_Image_vslfdxvslfdxvslf.png?raw=true)\n",
        "\n",
        "\n",
        "**The Cypher Strategy:**\n",
        "1.  **UNWIND:** We pass the entire list of tweets as a single parameter (`$batch`). `UNWIND` processes them one row at a time.\n",
        "2.  **MERGE (User, Place, Hashtag):** We use `MERGE` to find existing nodes or create them if they don't exist. This prevents duplicates.\n",
        "3.  **CREATE (Tweet):** We use `CREATE` for tweets because every tweet ID is globally unique; we always want a new node.\n",
        "4.  **Relationships:** We connect the entities as shown in the diagram above."
      ],
      "metadata": {
        "id": "jXHov2Hul-Mi"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sQ3qPnnvZHHN"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "from neo4j import GraphDatabase\n",
        "\n",
        "\n",
        "query = \"\"\"\n",
        "UNWIND $batch AS row\n",
        "\n",
        "// --------------------------------------------------------\n",
        "// 1. NODE: USER (Complete Profile Metrics)\n",
        "// --------------------------------------------------------\n",
        "MERGE (u:User {id: row.__expansion_author.id})\n",
        "SET u.username = row.__expansion_author.username,\n",
        "    u.name = row.__expansion_author.name,\n",
        "\n",
        "    // --- USER METRICS ---\n",
        "    u.followers = row.__expansion_author.public_metrics.followers_count,\n",
        "    u.following = row.__expansion_author.public_metrics.following_count,\n",
        "    u.tweet_count = row.__expansion_author.public_metrics.tweet_count,\n",
        "    u.listed_count = row.__expansion_author.public_metrics.listed_count // (If available)\n",
        "\n",
        "// --------------------------------------------------------\n",
        "// 2. NODE: TWEET (Complete Engagement Metrics)\n",
        "// --------------------------------------------------------\n",
        "CREATE (t:Tweet {id: row.id})\n",
        "SET t.text = row.text,\n",
        "    t.created_at = datetime(row.created_at),\n",
        "\n",
        "    // --- TWEET METRICS ---\n",
        "    t.likes = row.public_metrics.like_count,\n",
        "    t.retweets = row.public_metrics.retweet_count,\n",
        "    t.replies = row.public_metrics.reply_count,\n",
        "    t.quotes = row.public_metrics.quote_count,\n",
        "\n",
        "    // --- GEO PIN ---\n",
        "    t.location = point({\n",
        "        longitude: row.geo.coordinates.coordinates[0],\n",
        "        latitude:  row.geo.coordinates.coordinates[1]\n",
        "    })\n",
        "\n",
        "// Link User -> Tweet\n",
        "MERGE (u)-[:POSTED]->(t)\n",
        "\n",
        "// --------------------------------------------------------\n",
        "// 3. NODE: PLACE (City Center)\n",
        "// --------------------------------------------------------\n",
        "MERGE (p:Place {name: row.place.full_name})\n",
        "ON CREATE SET\n",
        "    p.country = row.place.country,\n",
        "    p.location = point({\n",
        "        longitude: row.place.centroid[0],\n",
        "        latitude:  row.place.centroid[1]\n",
        "    })\n",
        "\n",
        "// Link Tweet -> Place\n",
        "MERGE (t)-[:LOCATED_AT]->(p)\n",
        "\n",
        "// --------------------------------------------------------\n",
        "// 4. NODE: HASHTAG\n",
        "// --------------------------------------------------------\n",
        "FOREACH (tagObj IN row.entities.hashtags |\n",
        "    MERGE (h:Hashtag {name: toLower(tagObj.tag)})\n",
        "    MERGE (t)-[:TAGGED_WITH]->(h)\n",
        ")\n",
        "\"\"\"\n",
        "\n",
        "print(\"Importing Fully Loaded Graph...\")\n",
        "# Make sure to use the filename you generated in the last step\n",
        "with open(\"dummy_tweets.json\", \"r\") as f:\n",
        "    data = json.load(f)\n",
        "\n",
        "with GraphDatabase.driver(URI, auth=AUTH) as driver:\n",
        "    driver.verify_connectivity()\n",
        "    with driver.session(database=\"neo4j\") as session:\n",
        "        session.execute_write(lambda tx: tx.run(query, batch=data))\n",
        "\n",
        "print(\"‚úÖ Success! All properties (Followers, Tweet Count, etc.) are now in the graph.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### üèÅ Conclusion\n",
        "You have successfully built a pipeline that transforms raw JSON streams into a graph!\n",
        "\n",
        "**What's Next?**\n",
        "Now that your data is in Neo4j, you have a foundation for:\n",
        "* **Graph Analytics:** \"Who is the most influential user?\"\n",
        "* **Vector Search:** \"Find tweets about data science.\" (Using the semantic text we just generated!)"
      ],
      "metadata": {
        "id": "7tY4pCMMmDd3"
      }
    }
  ]
}